company,job-title,job-description,level
PepsiCo,Data Engineer,"OverviewPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.What PepsiCo Data Management and Operations does: Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders Increase awareness about available data and democratize access to it across the companyJob DescriptionAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, onpremise data sources as well as cloud and remote systems.ResponsibilitiesActive contributor to code development in projects and services.Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.Responsible for implementing best practices around systems integration, security, performance and data management.Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.Develop and optimize procedures to “productionalize” data science models.Define and manage SLA’s for data products and processes running in production.Support large-scale experimentation done by data scientists.Prototype new approaches and build solutions at scale.Research in state-of-the-art methodologies.Create documentation for learnings and knowledge transfer.Create and audit reusable packages or libraries.Qualifications4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).2+ years in cloud data engineering experience in Azure.Fluent with Azure cloud services. Azure Certification is a plus.Experience with integration of multi cloud services with on-premises technologies.Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.Experience with version control systems like Github and deployment & CI tools.Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools is a plus.Experience with Statistical/ML techniques is a plus.Experience with building solutions in the retail or in the supply chain space is a plusUnderstanding of metadata management, data lineage, and data glossaries is a plus.Working knowledge of agile development, including DevOps and DataOps concepts. Familiarity with business intelligence tools (such as PowerBI).EducationBA/BS in Computer Science, Math, Physics, or other technical fields.Skills, Abilities, KnowledgeExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.Proven track record of leading, mentoring data teams.Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.Ability to understand and translate business requirements into data and technical requirements. High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.Foster a team culture of accountability, communication, and self-management.Proactively drives impact and engagement while bringing others along.Consistently attain/exceed individual and team goals.Ability to lead others without direct authority in a matrixed environment.CompetenciesHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.Understands both the engineering and business side of the Data Products released.Places the user in the center of decision making.Teams up and collaborates for speed, agility, and innovation.Experience with and embraces agile methodologies.Strong negotiation and decision-making skill.Experience managing and working with globally distributed teamsCOVID-19 vaccination is a condition of employment for this role. Please note that all such company vaccine requirements provide the opportunity to request an approved accommodation or exemption under applicable lawEEO StatementAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender IdentityIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.Please view our Pay Transparency Statement
      

        Show more

        


        Show less",Mid-Senior level
Massachusetts Institute of Technology,Data Engineer,"Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.


        Show more

        


        Show less",Entry level
Bloom Insurance,Data Engineer I,"To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.Essential FunctionsDay to day functions include the following:Design data models and develop database structures in Microsoft SQL server.Write various database objects like stored procedures, functions, views, triggers for various front end applications.Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.Create database deployment packages for deploying changes.Identify & repair inconsistencies in data, database tuning, query optimization.Able to generate ad hoc data on demand.Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise mannerDevelop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.DocumentationOptimization recommendationsDay to day troubleshooting.NET Programming as neededEducation/ExperienceBA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experienceSolid experience with various versions of MS SQL Server and TSQL programmingMicrosoft Certified DBA a plusSkills/KnowledgeStrong experience in writing efficient SQL codeWorking knowledge of SQL Server Management Studio (SSMS)Knowledge of SQL Server Reporting Services (SSRS)Knowledge of SQL Server Integration Services (SSIS)Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plusKnowledge of data science technologies is a plusClear, concise communication skills, excellent organizational skillsHighly self-motivated and directedKeen attention to detailHigh level of work intensity in a team environmentHigh integrity and values-drivenEager for professional developmentExperience and understanding of source control management a plusWhat We OfferAt Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:Competitive compensation Comprehensive health benefits Long-term career growth and mentoring About BloomAs an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.Ascend Technology ™Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
      

        Show more

        


        Show less",Entry level
Johnson & Johnson,Data Engineer Summer Intern,"Job DescriptionAt Johnson & Johnson, we use technology and the power of teamwork to discover new ways to prevent and overcome the world’s the most significant healthcare challenges. Our Corporate, Consumer Health, Medical Devices, and Pharmaceutical teams leverage data, real-world insights, and creative minds to make life-changing healthcare products and medicines. We're disrupting outdated healthcare ecosystems and infusing them with transformative ideas to help people thrive throughout every stage of their lives. With a reach of more than a billion people every day, there’s no limit to the impact you can make here. Are you ready to reimagine healthcare?Here, your career breakthroughs will change the future of health, in all the best ways. And you’ll change, too. You’ll be inspired, and you’ll inspire people across the world to change how they care for themselves and those they love. Amplify your impact. Join us! Tasked with transforming data into a format that can be easily consumes by ML algorithms. Adept Database basics SQL, capable of basic data engineering techniques – data wrangling, data cleansing, data curation. Fluent in one of the programming languages R or Python.  Data Engineering skills, e.g., cleaning/organizing data, data preparation, data collection, data integration across different data sources, data storage, relational/non-relational database management.  Proficient in structured query language (SQL), NoSQL, MATLAB, and programming in Python or R.  Proficient in Applied Machine Learning – Experience with a variety of algorithms, Building Predictive Models, Cross-validating, Hypertuning and Deployment.  Experience with devising and implementing ML methods for protein or antibody modeling or design, or with deep learning methods that relate protein sequence, structure, property or function.  MD simulation software (e.g., Commercial tools like Schrodinger, or open-source software AMBER, GROMACS, etc.). Additionally, prior experience in homology modeling and structural refinement  Knowledgeable of AWS services including Redshift, RDS, EMR and EC2  Working knowledge using software and tools including big data tools like Kafka, Spark and Hadoop. At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.Primary LocationNA-US-Pennsylvania-Spring HouseOrganizationJanssen Research & Development, LLC (6084)Job FunctionAdministration


        Show more

        


        Show less",Not Applicable
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $90K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insuranceOptional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatmentSchedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
2Bridge Partners,Data Engineer,"Well known NY based Non-Profit seeks a Data Engineer.An ideal candidate will have strong data engineering skills with both SQL and Python.This position can be based out of NYC or be 100% remote.Compensation includes salary, excellent medical, dental, vision benefits, pto, 401k, a quality of life oriented work life balance, and lots of other benefits.ResponsibilitiesThis position will work within the enterprise data engineering team to continuously build, maintain, and improve the design, performance, reliability, scalability, security, and architecture of enterprise data products.Responsibilities include:Develop robust database solutions, data integration (ETL, ELT) packages, automation, performance monitoring, SQL coding, database tuning and optimizations, security management, capacity planning, database HA, and database DR solutions across required data platforms.Conduct in-depth data analysis to support data product design.Identify and resolve production database and data integration issues.Collaborates closely with data quality, application, and visualization teams to deliver high-quality data products.Participate in code review, QA, release, and continuous deployment processes.Qualifications:Bachelors Degree in Computer Science or other quantitative disciplines such as Science, Statistics, Economics or Mathematics.5+ years of progressive database development experience with the Microsoft SQL Server family suite.Experience with data modeling and data architecture principles for both analytics and transactional systems.Hands-on experience with SQL Server, Oracle, or other RDBMS required.Python ScriptingExperience with Cloud Data PlatformsPreferred Experience:Scripting experience in Powershell, C#, Java, and/or R.Experience in the Microsoft Azure technology stack is a plus.Experience with data analytics and visualization is a plus.Preferred Knowledge:Intermediate knowledge of OLTP and OLAP database designIntermediate knowledge of data modeling (normalized and dimensional)Intermediate knowledge of ETL, ELT architecture especially for real and near-real-time scenariosIntermediate knowledge of Data Architecture implementation patternsAnticipated salary for candidates living in the NY Metro market in the 140,000 to 160,000 range.


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Geopath,Data Engineer,"Applicants must be authorized to work for any employer in the United States. We are unable to sponsor, or take over sponsorship, of an employment visa at this time.About the roleReporting to the SVP, Data & Technology and Lead Data Engineer, the Data Engineer will play a critical role in supporting and advancing Geopath’s new audience measurement platform.Responsibilities:Build scalable functions, fault tolerant batch & real time data pipelines to validate/extract/transform/integrate large scale datasets inclusive of location and time series data, across multiple platformsReview current data processes to identify improvement areas: design and implement solutions to improve scalability, performance, cost efficiencies and data qualityExtend and optimize Geopath’s data platform, which spans across multiple cloud services, data warehouses, and applications in order to meet the industry’s evolving data needsResearch, evaluate, analyze and integrate new data sources and develop quick prototypes for new data productsWork closely with product owners, data architects, data scientists and application development teams to quickly define prototypes, and build new data productsDevelop a solid understanding of the nuances within Geopath’s foundational data sourcesSupport internal stakeholders including data, design, product and executive teams by assisting them with data-related technical issuesRequirements:Bachelor’s or master's degree in computer science, computer engineering, informatics, or equivalent fields3+ years of experience as a Data Engineer with demonstrated experience in data modeling, ETL, data warehouse/data lakes, and consolidating data from multiple data sources3+ years of experience with SQL, solid experience in writing stored procedures and UDFs, familiarity with Snowflake’s data warehouse a plus2+ years of experience in building and optimizing scalable and robust big data pipelines in Apache Airflow or other workflow engines and fluent in Python or Java programmingGood working knowledge in data partition and query optimizationExperience or desire to work in a start-up environment, good team player, and can work independently with minimal supervisionGreat analytical and problem-solving skills, eager to learn new technologies and willing to wear different hats in a small team settingGreat communication and organizational skillsExpected salary for this role is between $75,000-$140,000 per year, depending on experience.About Geopath Inc.Established in 1933, Geopath, (previously the Traffic Audit Bureau for Media Measurement Inc.,) has been the gold standard in providing media auditing and audience measurement services for the Out of Home industry in the US for 90 years. Geopath has now expanded its historical mission and is looking to the future by modernizing its mission critical, industry-standard media audience rating platform while embracing the digital era. In addition to being the industry’s media auditing solution, Geopath also analyzes people’s movement data, develops deep consumer insights, innovations and advanced media research methodologies to uncover critical insights about how consumers engage with Out of Home advertising across a multi-channel ecosystem and in the physical world.


        Show more

        


        Show less",Mid-Senior level
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insurance.Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
,,,
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insurance.Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
,,,
Bloom Insurance,Data Engineer I,"To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.Essential FunctionsDay to day functions include the following:Design data models and develop database structures in Microsoft SQL server.Write various database objects like stored procedures, functions, views, triggers for various front end applications.Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.Create database deployment packages for deploying changes.Identify & repair inconsistencies in data, database tuning, query optimization.Able to generate ad hoc data on demand.Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise mannerDevelop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.DocumentationOptimization recommendationsDay to day troubleshooting.NET Programming as neededEducation/ExperienceBA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experienceSolid experience with various versions of MS SQL Server and TSQL programmingMicrosoft Certified DBA a plusSkills/KnowledgeStrong experience in writing efficient SQL codeWorking knowledge of SQL Server Management Studio (SSMS)Knowledge of SQL Server Reporting Services (SSRS)Knowledge of SQL Server Integration Services (SSIS)Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plusKnowledge of data science technologies is a plusClear, concise communication skills, excellent organizational skillsHighly self-motivated and directedKeen attention to detailHigh level of work intensity in a team environmentHigh integrity and values-drivenEager for professional developmentExperience and understanding of source control management a plusWhat We OfferAt Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:Competitive compensation Comprehensive health benefits Long-term career growth and mentoring About BloomAs an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.Ascend Technology ™Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
      

        Show more

        


        Show less",Entry level
PepsiCo,Data Engineer,"OverviewPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.What PepsiCo Data Management and Operations does: Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders Increase awareness about available data and democratize access to it across the companyJob DescriptionAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, onpremise data sources as well as cloud and remote systems.ResponsibilitiesActive contributor to code development in projects and services.Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.Responsible for implementing best practices around systems integration, security, performance and data management.Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.Develop and optimize procedures to “productionalize” data science models.Define and manage SLA’s for data products and processes running in production.Support large-scale experimentation done by data scientists.Prototype new approaches and build solutions at scale.Research in state-of-the-art methodologies.Create documentation for learnings and knowledge transfer.Create and audit reusable packages or libraries.Qualifications4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).2+ years in cloud data engineering experience in Azure.Fluent with Azure cloud services. Azure Certification is a plus.Experience with integration of multi cloud services with on-premises technologies.Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.Experience with version control systems like Github and deployment & CI tools.Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools is a plus.Experience with Statistical/ML techniques is a plus.Experience with building solutions in the retail or in the supply chain space is a plusUnderstanding of metadata management, data lineage, and data glossaries is a plus.Working knowledge of agile development, including DevOps and DataOps concepts. Familiarity with business intelligence tools (such as PowerBI).EducationBA/BS in Computer Science, Math, Physics, or other technical fields.Skills, Abilities, KnowledgeExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.Proven track record of leading, mentoring data teams.Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.Ability to understand and translate business requirements into data and technical requirements. High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.Foster a team culture of accountability, communication, and self-management.Proactively drives impact and engagement while bringing others along.Consistently attain/exceed individual and team goals.Ability to lead others without direct authority in a matrixed environment.CompetenciesHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.Understands both the engineering and business side of the Data Products released.Places the user in the center of decision making.Teams up and collaborates for speed, agility, and innovation.Experience with and embraces agile methodologies.Strong negotiation and decision-making skill.Experience managing and working with globally distributed teamsCOVID-19 vaccination is a condition of employment for this role. Please note that all such company vaccine requirements provide the opportunity to request an approved accommodation or exemption under applicable lawEEO StatementAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender IdentityIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.Please view our Pay Transparency Statement
      

        Show more

        


        Show less",Mid-Senior level
Johnson & Johnson,Data Engineer Summer Intern,"Job DescriptionAt Johnson & Johnson, we use technology and the power of teamwork to discover new ways to prevent and overcome the world’s the most significant healthcare challenges. Our Corporate, Consumer Health, Medical Devices, and Pharmaceutical teams leverage data, real-world insights, and creative minds to make life-changing healthcare products and medicines. We're disrupting outdated healthcare ecosystems and infusing them with transformative ideas to help people thrive throughout every stage of their lives. With a reach of more than a billion people every day, there’s no limit to the impact you can make here. Are you ready to reimagine healthcare?Here, your career breakthroughs will change the future of health, in all the best ways. And you’ll change, too. You’ll be inspired, and you’ll inspire people across the world to change how they care for themselves and those they love. Amplify your impact. Join us! Tasked with transforming data into a format that can be easily consumes by ML algorithms. Adept Database basics SQL, capable of basic data engineering techniques – data wrangling, data cleansing, data curation. Fluent in one of the programming languages R or Python.  Data Engineering skills, e.g., cleaning/organizing data, data preparation, data collection, data integration across different data sources, data storage, relational/non-relational database management.  Proficient in structured query language (SQL), NoSQL, MATLAB, and programming in Python or R.  Proficient in Applied Machine Learning – Experience with a variety of algorithms, Building Predictive Models, Cross-validating, Hypertuning and Deployment.  Experience with devising and implementing ML methods for protein or antibody modeling or design, or with deep learning methods that relate protein sequence, structure, property or function.  MD simulation software (e.g., Commercial tools like Schrodinger, or open-source software AMBER, GROMACS, etc.). Additionally, prior experience in homology modeling and structural refinement  Knowledgeable of AWS services including Redshift, RDS, EMR and EC2  Working knowledge using software and tools including big data tools like Kafka, Spark and Hadoop. At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.Primary LocationNA-US-Pennsylvania-Spring HouseOrganizationJanssen Research & Development, LLC (6084)Job FunctionAdministration


        Show more

        


        Show less",Not Applicable
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $90K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insuranceOptional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatmentSchedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
2Bridge Partners,Data Engineer,"Well known NY based Non-Profit seeks a Data Engineer.An ideal candidate will have strong data engineering skills with both SQL and Python.This position can be based out of NYC or be 100% remote.Compensation includes salary, excellent medical, dental, vision benefits, pto, 401k, a quality of life oriented work life balance, and lots of other benefits.ResponsibilitiesThis position will work within the enterprise data engineering team to continuously build, maintain, and improve the design, performance, reliability, scalability, security, and architecture of enterprise data products.Responsibilities include:Develop robust database solutions, data integration (ETL, ELT) packages, automation, performance monitoring, SQL coding, database tuning and optimizations, security management, capacity planning, database HA, and database DR solutions across required data platforms.Conduct in-depth data analysis to support data product design.Identify and resolve production database and data integration issues.Collaborates closely with data quality, application, and visualization teams to deliver high-quality data products.Participate in code review, QA, release, and continuous deployment processes.Qualifications:Bachelors Degree in Computer Science or other quantitative disciplines such as Science, Statistics, Economics or Mathematics.5+ years of progressive database development experience with the Microsoft SQL Server family suite.Experience with data modeling and data architecture principles for both analytics and transactional systems.Hands-on experience with SQL Server, Oracle, or other RDBMS required.Python ScriptingExperience with Cloud Data PlatformsPreferred Experience:Scripting experience in Powershell, C#, Java, and/or R.Experience in the Microsoft Azure technology stack is a plus.Experience with data analytics and visualization is a plus.Preferred Knowledge:Intermediate knowledge of OLTP and OLAP database designIntermediate knowledge of data modeling (normalized and dimensional)Intermediate knowledge of ETL, ELT architecture especially for real and near-real-time scenariosIntermediate knowledge of Data Architecture implementation patternsAnticipated salary for candidates living in the NY Metro market in the 140,000 to 160,000 range.


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
,,,
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Bloom Insurance,Data Engineer I,"To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.Essential FunctionsDay to day functions include the following:Design data models and develop database structures in Microsoft SQL server.Write various database objects like stored procedures, functions, views, triggers for various front end applications.Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.Create database deployment packages for deploying changes.Identify & repair inconsistencies in data, database tuning, query optimization.Able to generate ad hoc data on demand.Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise mannerDevelop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.DocumentationOptimization recommendationsDay to day troubleshooting.NET Programming as neededEducation/ExperienceBA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experienceSolid experience with various versions of MS SQL Server and TSQL programmingMicrosoft Certified DBA a plusSkills/KnowledgeStrong experience in writing efficient SQL codeWorking knowledge of SQL Server Management Studio (SSMS)Knowledge of SQL Server Reporting Services (SSRS)Knowledge of SQL Server Integration Services (SSIS)Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plusKnowledge of data science technologies is a plusClear, concise communication skills, excellent organizational skillsHighly self-motivated and directedKeen attention to detailHigh level of work intensity in a team environmentHigh integrity and values-drivenEager for professional developmentExperience and understanding of source control management a plusWhat We OfferAt Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:Competitive compensation Comprehensive health benefits Long-term career growth and mentoring About BloomAs an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.Ascend Technology ™Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
      

        Show more

        


        Show less",Entry level
Johnson & Johnson,Data Engineer Summer Intern,"Job DescriptionAt Johnson & Johnson, we use technology and the power of teamwork to discover new ways to prevent and overcome the world’s the most significant healthcare challenges. Our Corporate, Consumer Health, Medical Devices, and Pharmaceutical teams leverage data, real-world insights, and creative minds to make life-changing healthcare products and medicines. We're disrupting outdated healthcare ecosystems and infusing them with transformative ideas to help people thrive throughout every stage of their lives. With a reach of more than a billion people every day, there’s no limit to the impact you can make here. Are you ready to reimagine healthcare?Here, your career breakthroughs will change the future of health, in all the best ways. And you’ll change, too. You’ll be inspired, and you’ll inspire people across the world to change how they care for themselves and those they love. Amplify your impact. Join us! Tasked with transforming data into a format that can be easily consumes by ML algorithms. Adept Database basics SQL, capable of basic data engineering techniques – data wrangling, data cleansing, data curation. Fluent in one of the programming languages R or Python.  Data Engineering skills, e.g., cleaning/organizing data, data preparation, data collection, data integration across different data sources, data storage, relational/non-relational database management.  Proficient in structured query language (SQL), NoSQL, MATLAB, and programming in Python or R.  Proficient in Applied Machine Learning – Experience with a variety of algorithms, Building Predictive Models, Cross-validating, Hypertuning and Deployment.  Experience with devising and implementing ML methods for protein or antibody modeling or design, or with deep learning methods that relate protein sequence, structure, property or function.  MD simulation software (e.g., Commercial tools like Schrodinger, or open-source software AMBER, GROMACS, etc.). Additionally, prior experience in homology modeling and structural refinement  Knowledgeable of AWS services including Redshift, RDS, EMR and EC2  Working knowledge using software and tools including big data tools like Kafka, Spark and Hadoop. At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.Primary LocationNA-US-Pennsylvania-Spring HouseOrganizationJanssen Research & Development, LLC (6084)Job FunctionAdministration


        Show more

        


        Show less",Not Applicable
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $90K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insuranceOptional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatmentSchedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
2Bridge Partners,Data Engineer,"Well known NY based Non-Profit seeks a Data Engineer.An ideal candidate will have strong data engineering skills with both SQL and Python.This position can be based out of NYC or be 100% remote.Compensation includes salary, excellent medical, dental, vision benefits, pto, 401k, a quality of life oriented work life balance, and lots of other benefits.ResponsibilitiesThis position will work within the enterprise data engineering team to continuously build, maintain, and improve the design, performance, reliability, scalability, security, and architecture of enterprise data products.Responsibilities include:Develop robust database solutions, data integration (ETL, ELT) packages, automation, performance monitoring, SQL coding, database tuning and optimizations, security management, capacity planning, database HA, and database DR solutions across required data platforms.Conduct in-depth data analysis to support data product design.Identify and resolve production database and data integration issues.Collaborates closely with data quality, application, and visualization teams to deliver high-quality data products.Participate in code review, QA, release, and continuous deployment processes.Qualifications:Bachelors Degree in Computer Science or other quantitative disciplines such as Science, Statistics, Economics or Mathematics.5+ years of progressive database development experience with the Microsoft SQL Server family suite.Experience with data modeling and data architecture principles for both analytics and transactional systems.Hands-on experience with SQL Server, Oracle, or other RDBMS required.Python ScriptingExperience with Cloud Data PlatformsPreferred Experience:Scripting experience in Powershell, C#, Java, and/or R.Experience in the Microsoft Azure technology stack is a plus.Experience with data analytics and visualization is a plus.Preferred Knowledge:Intermediate knowledge of OLTP and OLAP database designIntermediate knowledge of data modeling (normalized and dimensional)Intermediate knowledge of ETL, ELT architecture especially for real and near-real-time scenariosIntermediate knowledge of Data Architecture implementation patternsAnticipated salary for candidates living in the NY Metro market in the 140,000 to 160,000 range.


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
,,,
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
,,,
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
,,,
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
Massachusetts Institute of Technology,Software and Data Junior Engineer,"Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.


        Show more

        


        Show less",Mid-Senior level
Massachusetts Institute of Technology,Data Engineer,"Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.


        Show more

        


        Show less",Entry level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $90K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insuranceOptional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatmentSchedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
Lakeshore Learning Materials,Junior Data Engineer (Remote),"Company DescriptionAt Lakeshore, we create innovative learning materials and world-class guest experiences for teachers, parents and children. Since 1954, we’ve grown into a global community—with a thriving e-commerce business, multiple catalogs, 60+ retail stores, a peerless national sales force, plus international offices that support our preeminent supply chain division. But today we’re working better, smarter and faster than ever—and setting our sights even higher. We’re building an infrastructure designed for scalability, embracing data-driven decision-making and using technology to improve efficiency and ensure the best tools for the best work. Most importantly, we continue to invest in a diverse team of inquisitive top talent who fuel each other’s passions and curiosity, take risks, try new things, and believe that every new day brings opportunities for growth.Job DescriptionIn a time of unprecedented expansion, are seeking a Junior Data Engineer who will be responsible for enabling and enhancing data insights for internal stakeholders, with significant overlap and collaboration with different teams. In this role, you will design, build, enhance and maintain an enterprise business intelligence (BI) platform that supports advanced analytics. The position also requires data cleanup, requirements gathering, data modeling and transformation, reports/dashboard creation, data mining and analytics on complex data sets. The ideal candidate has proven experience working with complex data and has a passion for improving technology. And we’re here to help you succeed—in 2022 Lakeshore earned its Great Place to Work® Certification™ and is proud to put its associates first.A day in the office looks like this: Serving as the primary point of contact with business teams to provide actionable insights into current delivery performance, as well as ad hoc investigations into future improvements or innovations Using BI and visualization software (SQL Server, Redshift, AWS Data Lake, Qlik, etc.) to empower nontechnical, internal customers to drive their own analytics and reporting Providing complex analysis, conceptualization, design, implementation and development of solutions for critical BI components Performing dataflow, system and data analysis; developing meaningful presentation of data in BI applications Contributing to data analysis, design and development of new and ongoing BI projects Collaborating closely with internal and external teams to understand modifications impacting data lake, visualization, etc. Participating in the entire lifecycle of BI solution delivery Helping plan, design, implement and manage the deployment of a self-service data visualization platform in Qlik Building and maintaining data visualizations that inform and engage business stakeholders Analyzing key performance indicators to discover root causes for various parameters QualificationsGot the skills and experience? Here’s what we’re looking for:At least 1 year of experience in relevant business domains, including data warehousing and BI tools, techniques and technology Knowledge of data warehouse platforms such as SQL Server, AWS Redshift and SSIS required Knowledge of Qlik and Tableau as visualization tools required Knowledge of ETL, presentation layer and design strategy reporting required Knowledge of data mining and experience using large-scale, complex data sets in a business environment Proficiency in SQL and performance optimization Technical capability to query large data sets and apply statistical models Knowledge of advanced statistics and experience with statistical data analysis systems (scikit-learn, Pandas) a plus Additional InformationAnd here’s our end of the bargain! Hourly: $27-$31 with an annual bonus of up to 10%Excellent medical/dental and vision coverage—EPO, PPO and HSA401(k) retirement plan with company contribution (because you will retire someday)Flexible benefits—choose what you like, ignore the restOn-site preschool for our employees’ childrenOn-site employee gym for all levels/fitness needsGenerous employee discount on products that make you smarterCasual dress…and we really mean itAt Lakeshore, we know our diversity makes us stronger, and when everyone feels included and valued, we all win. We strive to embrace our differences and create an intentionally diverse and inclusive community that is representative of the teachers, families and children we serve.We know we couldn’t do the extraordinary things we’re doing without the people on our team. Thanks to the passion and enthusiasm of this spectacular group, Lakeshore is more than a great place to work—it’s a great experience to be part of. Day in and day out, we give everything we’ve got to create products that instill a sense of wonder and foster a true love of learning. To help maintain this high bar for success, we’re constantly on the lookout for people to join us. So if you’re a down-to-earth professional who shares our desire for making a difference, we’d love to hear from you.To learn more about Lakeshore, visit www.lakeshorelearning.com/careersEqual Employment Opportunity PolicyPeople are selected to become members of the Lakeshore family based on skill, merit and mind-boggling talent—not based on race, color, creed, sexual orientation, gender or gender identity, marital status, domestic partnership status, military status, religion, age, national origin, ancestry, alienage, AIDS or AIDS-related complex status, genetic information, predisposition or carrier status, status as a victim of domestic violence, physical or mental disability, or any other characteristic protected by applicable law. If things aren’t equal, we all lose.To learn about how we collect and use Applicant information, please visit our Employee/Applicant Privacy Policy. INDRLL10
      

        Show more

        


        Show less",Associate
Massachusetts Institute of Technology,Data Engineer,"Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.


        Show more

        


        Show less",Entry level
Bloom Insurance,Data Engineer I,"To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.Essential FunctionsDay to day functions include the following:Design data models and develop database structures in Microsoft SQL server.Write various database objects like stored procedures, functions, views, triggers for various front end applications.Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.Create database deployment packages for deploying changes.Identify & repair inconsistencies in data, database tuning, query optimization.Able to generate ad hoc data on demand.Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise mannerDevelop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.DocumentationOptimization recommendationsDay to day troubleshooting.NET Programming as neededEducation/ExperienceBA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experienceSolid experience with various versions of MS SQL Server and TSQL programmingMicrosoft Certified DBA a plusSkills/KnowledgeStrong experience in writing efficient SQL codeWorking knowledge of SQL Server Management Studio (SSMS)Knowledge of SQL Server Reporting Services (SSRS)Knowledge of SQL Server Integration Services (SSIS)Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plusKnowledge of data science technologies is a plusClear, concise communication skills, excellent organizational skillsHighly self-motivated and directedKeen attention to detailHigh level of work intensity in a team environmentHigh integrity and values-drivenEager for professional developmentExperience and understanding of source control management a plusWhat We OfferAt Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:Competitive compensation Comprehensive health benefits Long-term career growth and mentoring About BloomAs an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.Ascend Technology ™Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
      

        Show more

        


        Show less",Entry level
PepsiCo,Data Engineer,"OverviewPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.What PepsiCo Data Management and Operations does: Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders Increase awareness about available data and democratize access to it across the companyJob DescriptionAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, onpremise data sources as well as cloud and remote systems.ResponsibilitiesActive contributor to code development in projects and services.Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.Responsible for implementing best practices around systems integration, security, performance and data management.Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.Develop and optimize procedures to “productionalize” data science models.Define and manage SLA’s for data products and processes running in production.Support large-scale experimentation done by data scientists.Prototype new approaches and build solutions at scale.Research in state-of-the-art methodologies.Create documentation for learnings and knowledge transfer.Create and audit reusable packages or libraries.Qualifications4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).2+ years in cloud data engineering experience in Azure.Fluent with Azure cloud services. Azure Certification is a plus.Experience with integration of multi cloud services with on-premises technologies.Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.Experience with version control systems like Github and deployment & CI tools.Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools is a plus.Experience with Statistical/ML techniques is a plus.Experience with building solutions in the retail or in the supply chain space is a plusUnderstanding of metadata management, data lineage, and data glossaries is a plus.Working knowledge of agile development, including DevOps and DataOps concepts. Familiarity with business intelligence tools (such as PowerBI).EducationBA/BS in Computer Science, Math, Physics, or other technical fields.Skills, Abilities, KnowledgeExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.Proven track record of leading, mentoring data teams.Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.Ability to understand and translate business requirements into data and technical requirements. High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.Foster a team culture of accountability, communication, and self-management.Proactively drives impact and engagement while bringing others along.Consistently attain/exceed individual and team goals.Ability to lead others without direct authority in a matrixed environment.CompetenciesHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.Understands both the engineering and business side of the Data Products released.Places the user in the center of decision making.Teams up and collaborates for speed, agility, and innovation.Experience with and embraces agile methodologies.Strong negotiation and decision-making skill.Experience managing and working with globally distributed teamsCOVID-19 vaccination is a condition of employment for this role. Please note that all such company vaccine requirements provide the opportunity to request an approved accommodation or exemption under applicable lawEEO StatementAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender IdentityIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.Please view our Pay Transparency Statement
      

        Show more

        


        Show less",Mid-Senior level
LogixHealth,Data Engineer,"Location:Bedford, MA; HybridThis Role:As a Data Engineer at LogixHealth, you will work with a globally distributed team of engineers to design, build and maintain cutting edge solutions that will directly improve the healthcare industry. You’ll contribute to our fast-paced, collaborative environment and will bring your expertise to continue delivering innovative technology solutions. The ideal candidate will be able to develop large scale, distributed data pipelines with an eye towards data security, availability and quality. The candidate should be experienced with modern data storage & transmission techniques, big data tools and distributed processes. The candidate should have excellent interpersonal communication and an aptitude to continue learning.Key Responsibilities:Design highly scalable, available and fault tolerant data processing systemsBuild out data quality proceduresCollaborate with other engineers on existing software and data integration solutionsKeep up with the latest technology industry trends and innovations Help other team members learn and adopt new technologies and practicesQuickly learn new and existing technologiesQualifications:To perform this job successfully, an individual must be able to perform each Key Responsibility satisfactorily. The following requirements are representative of the knowledge, skills, and/or ability required to perform this job successfully. Reasonable accommodation may be made to enable individuals with disabilities to perform the duties. Required:BS/MS in Computer Science, related technical field or equivalent experienceStrong programming and scripting skillsData modeling and data warehousing/lakesREST API servicesExpertise in data storage systems, including SQL & NoSQL database systems & file object storageAdvanced SQL and query performance tuning skillsUnderstanding of cloud computing technologies & platformsExperience with gitExcellent interpersonal communication skillsPreferred:Big data analysis techniquesBig data visualization solutionsDistributed systems design Healthcare industry knowledgeBenefits at LogixHealth:We offer a comprehensive benefits package including health, dental and vision, 401(k), PTO, paid holidays, life and disability insurance, on-site fitness center and company-wide social events.About LogixHealth:At LogixHealth we provide expert coding and billing services that allow physicians to focus on providing great clinical care. LogixHealth was founded in the 1990s by physicians to service their own practices and has grown to become the nation’s leading provider of unsurpassed software-enabled revenue cycle management services, offering a complete range of solutions, including coding and claims management and the latest business intelligence reporting dashboards for clients in 40 states.Since our first day, we have had a clear vision of a better healthcare system and have continually evolved to get there. In addition to providing expert revenue cycle services, we utilize proprietary software to provide valuable financial, clinical, and other data insights that directly improve the quality and efficiency of patient care.At LogixHealth, we’re committed to Making intelligence matter through our pillars of Physician-Inspired Knowledge, Unrivaled Technology and Impeccable Service.To learn more about us, visit our website https://www.logixhealth.com/.


        Show more

        


        Show less",Entry level
Johnson & Johnson,Data Engineer Summer Intern,"Job DescriptionAt Johnson & Johnson, we use technology and the power of teamwork to discover new ways to prevent and overcome the world’s the most significant healthcare challenges. Our Corporate, Consumer Health, Medical Devices, and Pharmaceutical teams leverage data, real-world insights, and creative minds to make life-changing healthcare products and medicines. We're disrupting outdated healthcare ecosystems and infusing them with transformative ideas to help people thrive throughout every stage of their lives. With a reach of more than a billion people every day, there’s no limit to the impact you can make here. Are you ready to reimagine healthcare?Here, your career breakthroughs will change the future of health, in all the best ways. And you’ll change, too. You’ll be inspired, and you’ll inspire people across the world to change how they care for themselves and those they love. Amplify your impact. Join us! Tasked with transforming data into a format that can be easily consumes by ML algorithms. Adept Database basics SQL, capable of basic data engineering techniques – data wrangling, data cleansing, data curation. Fluent in one of the programming languages R or Python.  Data Engineering skills, e.g., cleaning/organizing data, data preparation, data collection, data integration across different data sources, data storage, relational/non-relational database management.  Proficient in structured query language (SQL), NoSQL, MATLAB, and programming in Python or R.  Proficient in Applied Machine Learning – Experience with a variety of algorithms, Building Predictive Models, Cross-validating, Hypertuning and Deployment.  Experience with devising and implementing ML methods for protein or antibody modeling or design, or with deep learning methods that relate protein sequence, structure, property or function.  MD simulation software (e.g., Commercial tools like Schrodinger, or open-source software AMBER, GROMACS, etc.). Additionally, prior experience in homology modeling and structural refinement  Knowledgeable of AWS services including Redshift, RDS, EMR and EC2  Working knowledge using software and tools including big data tools like Kafka, Spark and Hadoop. At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.Primary LocationNA-US-Pennsylvania-Spring HouseOrganizationJanssen Research & Development, LLC (6084)Job FunctionAdministration


        Show more

        


        Show less",Not Applicable
Pura,Data Engineer (Mid/Jr),"Data Engineer (Mid/Jr Level)Pura has been revolutionizing the smart home experience for the past several years. We obsess over providing world-class experiences for our customers, partners, and vendors. We pride ourselves on maintaining a high standard of quality and innovation with our products, and continuous growth and development for our people.We are looking for a Data Engineer to help business users and analysts throughout the organization access the data they need to operate and grow the business.What you’ll own:In this high-impact role, you will:Work closely with the Data Science team to design and develop scalable data pipelines for processing and analyzing large volumes of dataBuild and maintain ETL processes using Python, SQL, Apache Airflow, and other technologiesDevelop and deploy data processing jobs on AWS or GCP using Docker and KubernetesWrite API wrappers to integrate with various external data sources and third-party toolsImplement and maintain best practices for data security, data quality, and data governanceCollaborate with other cross-functional teams to ensure data is available, reliable, and accessible to support business decisionsWrite clean, readable, and maintainable code and ensure code is thoroughly tested and documentedQualifications:Bachelor's degree in Computer Science, Software Engineering, or related field1-3 years of experience in data engineering or a related fieldProficiency in Python, SQL, Apache Airflow, and DockerExperience with AWS or GCP and some Kubernetes experienceStrong analytical and problem-solving skillsExcellent communication and collaboration skillsAbility to work independently and as part of a teamPassion for writing clean, readable code and ensuring code qualityIf you are passionate about data engineering and want to join a fast-paced, dynamic team that is making a real impact, we encourage you to apply today!.Pura’s StoryAt Pura, we’re pairing smart tech with premium fragrance to create a perfectly personalized and customized scenting experience for the individual. We partner with brands like Disney, Capri Blue, and Anthropologie to bring original and well-loved fragrances to homes in a modern, convenient, and safe way. We know we’ve only just begun to unlock the possibility of scent, and we’re excited for the opportunities that lie ahead.We’re quickly turning heads and getting noticed. We raised a seed round of 4.4M in February of 2020, was recognized by Inc. Magazine as a 2021 Best Workplace, won the Silicon Slopes Hall of Fame & Awards Advertising category in 2022, and we’re currently the 6th-fastest growing company in Utah. Check out our Instagram @pura and TikTok @trypura channels for a look into the excited, engaged community we’re building. We pride ourselves on being a human brand and in creating a culture worth talking about, and we have big goals for the future.Join the Pura Team!All candidates are subject to a background check.Pura provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Powered by JazzHRPSRu221guI
      

        Show more

        


        Show less",Mid-Senior level
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Pulivarthi Group (PG),Data Engineer,"Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/Pulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.We’ve served some of the largest healthcare, financial services, and government entities in the U.S.Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/Data Engineer - Experience in data engineering and building applications; Experience in Python/PySpark; Experience in Typescript (Preferred) or JavascriptExperience in building applications or dashboards using no- and low-code tools.Coding Skills: Python complete language proficiency; SQL proficiency in querying language (join types, filtering, aggregation) and data modeling (relationship types, constraints);PySpark basic familiarity (DataFrame operations, PySpark SQL functions) and differences with other DataFrame implementations (Pandas);Typescript experience in TypeScript or Javascript;Application building working with no- and low-code tools to query databases, define variables, filters, cross-filters, responsive front-end and user based applications.Databases familiarity with common relational database models and proprietary instantiations, such as SAP, Salesforce etc.; Git knowledge of version control / collaboration workflows and best practices; Agile familiarity with agile and iterative working methodology and rapid user feedback gathering concepts; UX design knowledge of best practices and applications; Data literacy data analysis and statistical basics to ensure correctness in data aggregation and visualization.
      

        Show more

        


        Show less",Mid-Senior level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
,,,
,,,
,,,
,,,
RemX - Accounting & Finance Staffing,Data Engineer,"New Opportunity Systems Engineer Greeneville, TN On-site $110-120k plus excellent benefits We are looking for an enthusiastic Systems Engineer to design, develop and install software solutions. The successful candidate will be able to review software applications standards and technical design. Implement new software platforms work with third party vendors. Support and/or install software applications/operating systems. Participate in the testing process through test review and analysis, test witnessing and certification of software. Requires a bachelor's degree in a related area and 2-5years of experience in the field or in a related area. Has knowledge of commonly used concepts, practices, and procedures within a particular field. Rely on instructions and pre-established guidelines to perform the functions of the job. Work as subject manner expert with minimum supervision. Primary job functions will exercise independent judgment when required. Typically reports to a department manager. Responsibilities: * Performance tuning, improvement, balancing, usability, automation * Support, maintain and document software functionality * Integrate new systems with existing systems * Evaluate and identify innovative technologies for implementation * Project planning and Project management * Maintain standards compliance * Implement new software platforms work with third party vendors * Document and demonstrates solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code * Improve operations by conducting systems analysis, recommending changes in policies and procedures * Protect operations by keeping information confidential * Provide information by collecting, analyzing, and summarizing development and service issues * Accomplish engineering and organization mission by completing related results as needed * Develop software/system solutions by studying information needs; conferring with users; studying systems flow, data usage and work processes; investigating problem areas; following the software/system development lifecycle. * Produce specifications and determine operational feasibility * Document and maintain software functionality * Serve as a subject matter expert * Comply with project plans and industry standards * * Requirements: Software Engineer top skills & proficiency: * Proven work experience in software engineering and/or Database Management * Firsthand experience in implementing and maintaining software/system applications * Firsthand experience in Relational Databases, SQL and etc. * Knowledge of ERP Systems * Experience with test-driven development * Ability to document requirements and specifications * Familiarity with software/system development methodology and release processes * Installing and configuring operating systems and application software * BS degree in Computer Science or relevant degree in Information Systems * Proficient in SQL Queries, stored procedures and working with in relational databases like SQL Server, MySQL, and ERP Systems * Analytical & Problem-Solving Skills * Ability to Learn Quickly * Team Player * Project Management * Written and Verbal Communication * Customer-Oriented * Analysis * General Programming Skills * SharePoint, MS Dynamics, HTML, and other related software a PLUS Company Description Each and every day RemX puts over 90,000 people to work, helping more than 15,000 companies find the talent they need in order to succeed. And, as a part of the 10th largest staffing company in the world, we understand that at the heart of every successful business are people. That's why we work hard to find you the right job at the right company. Explore all the exciting opportunities that RemX offers and find the right fit for you!Each and every day RemX puts over 90,000 people to work, helping more than 15,000 companies find the talent they need in order to succeed. And, as a part of the 10th largest staffing company in the world, we understand that at the heart of every successful business are people. That’s why we work hard to find you the right job at the right company. Explore all the exciting opportunities that RemX offers and find the right fit for you!
      

        Show more

        


        Show less",Entry level
,,,
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $90K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insuranceOptional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatmentSchedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
2Bridge Partners,Data Engineer,"Well known NY based Non-Profit seeks a Data Engineer.An ideal candidate will have strong data engineering skills with both SQL and Python.This position can be based out of NYC or be 100% remote.Compensation includes salary, excellent medical, dental, vision benefits, pto, 401k, a quality of life oriented work life balance, and lots of other benefits.ResponsibilitiesThis position will work within the enterprise data engineering team to continuously build, maintain, and improve the design, performance, reliability, scalability, security, and architecture of enterprise data products.Responsibilities include:Develop robust database solutions, data integration (ETL, ELT) packages, automation, performance monitoring, SQL coding, database tuning and optimizations, security management, capacity planning, database HA, and database DR solutions across required data platforms.Conduct in-depth data analysis to support data product design.Identify and resolve production database and data integration issues.Collaborates closely with data quality, application, and visualization teams to deliver high-quality data products.Participate in code review, QA, release, and continuous deployment processes.Qualifications:Bachelors Degree in Computer Science or other quantitative disciplines such as Science, Statistics, Economics or Mathematics.5+ years of progressive database development experience with the Microsoft SQL Server family suite.Experience with data modeling and data architecture principles for both analytics and transactional systems.Hands-on experience with SQL Server, Oracle, or other RDBMS required.Python ScriptingExperience with Cloud Data PlatformsPreferred Experience:Scripting experience in Powershell, C#, Java, and/or R.Experience in the Microsoft Azure technology stack is a plus.Experience with data analytics and visualization is a plus.Preferred Knowledge:Intermediate knowledge of OLTP and OLAP database designIntermediate knowledge of data modeling (normalized and dimensional)Intermediate knowledge of ETL, ELT architecture especially for real and near-real-time scenariosIntermediate knowledge of Data Architecture implementation patternsAnticipated salary for candidates living in the NY Metro market in the 140,000 to 160,000 range.


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Geopath,Data Engineer,"Applicants must be authorized to work for any employer in the United States. We are unable to sponsor, or take over sponsorship, of an employment visa at this time.About the roleReporting to the SVP, Data & Technology and Lead Data Engineer, the Data Engineer will play a critical role in supporting and advancing Geopath’s new audience measurement platform.Responsibilities:Build scalable functions, fault tolerant batch & real time data pipelines to validate/extract/transform/integrate large scale datasets inclusive of location and time series data, across multiple platformsReview current data processes to identify improvement areas: design and implement solutions to improve scalability, performance, cost efficiencies and data qualityExtend and optimize Geopath’s data platform, which spans across multiple cloud services, data warehouses, and applications in order to meet the industry’s evolving data needsResearch, evaluate, analyze and integrate new data sources and develop quick prototypes for new data productsWork closely with product owners, data architects, data scientists and application development teams to quickly define prototypes, and build new data productsDevelop a solid understanding of the nuances within Geopath’s foundational data sourcesSupport internal stakeholders including data, design, product and executive teams by assisting them with data-related technical issuesRequirements:Bachelor’s or master's degree in computer science, computer engineering, informatics, or equivalent fields3+ years of experience as a Data Engineer with demonstrated experience in data modeling, ETL, data warehouse/data lakes, and consolidating data from multiple data sources3+ years of experience with SQL, solid experience in writing stored procedures and UDFs, familiarity with Snowflake’s data warehouse a plus2+ years of experience in building and optimizing scalable and robust big data pipelines in Apache Airflow or other workflow engines and fluent in Python or Java programmingGood working knowledge in data partition and query optimizationExperience or desire to work in a start-up environment, good team player, and can work independently with minimal supervisionGreat analytical and problem-solving skills, eager to learn new technologies and willing to wear different hats in a small team settingGreat communication and organizational skillsExpected salary for this role is between $75,000-$140,000 per year, depending on experience.About Geopath Inc.Established in 1933, Geopath, (previously the Traffic Audit Bureau for Media Measurement Inc.,) has been the gold standard in providing media auditing and audience measurement services for the Out of Home industry in the US for 90 years. Geopath has now expanded its historical mission and is looking to the future by modernizing its mission critical, industry-standard media audience rating platform while embracing the digital era. In addition to being the industry’s media auditing solution, Geopath also analyzes people’s movement data, develops deep consumer insights, innovations and advanced media research methodologies to uncover critical insights about how consumers engage with Out of Home advertising across a multi-channel ecosystem and in the physical world.


        Show more

        


        Show less",Mid-Senior level
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insurance.Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
UCLA Health,Junior Software Engineer -  Jonsson Cancer Center,"DescriptionThe newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Center is seeking a programmer/analyst with research and development experience. This position will be working with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer. Our team is passionate about applying their knowledge of software-development and design to improve scientific research. We develop scalable and distributed software solutions that maximize utilization of both local high-performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. This requires a strong background in computer science, providing a platform for technical leadership, but linked to strong personal communication and leadership skills, to help ensure insights are broadly adopted. The successful candidate will be helping us perform research that will transform the lives of cancer patients. Your responsibilities will be to use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be rewsponsible for one major and several minor projects at any point in time. We are in a rapid growth-phase, and the successful candidate will be involved in hiring of new team members. Beyond your strong inter-personal skills and computer science background, you will have experience with either systems software, databases or algorithms, linked to implementation skills at least one of C++, R, Perl or Python. You will be comfortable in UNIX/Linux environments and using continuous integration and CASE tools. Salary Range: $5525-$10925 Monthly
      

        Show more

        


        Show less",Entry level
Diverse Lynx,Jr Data Engineer,"Job DescriptionSkills (Must Have):Python – Ingestion/manipulation of large datasets to S3 using pandasPython – Consumption of data from REST APIs using requestsAny language (Most preferably Python) – Small automation tasks within AWS S3, Glue, AthenaExperience developing in AWS. Key services: S3, Lambda, Athena, Step Functions, GlueGeneral familiarity with a variety of other systems such as Oracle/Postgres, REST APIs, SplunkDeployment of resources to AWS using ServerlessGeneral understand of Infrastructure as CodeSkills (Nice To Have)AI/Client experience in SagemakerGeneral knowledge of cyber security practices and frameworksExperience writing complex queries in Presto/HadoopDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.


        Show more

        


        Show less",Mid-Senior level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insurance.Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
,,,
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
,,,
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
,,,
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
UCLA Health,Junior Software Engineer -  Jonsson Cancer Center,"DescriptionThe newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Center is seeking a programmer/analyst with research and development experience. This position will be working with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer. Our team is passionate about applying their knowledge of software-development and design to improve scientific research. We develop scalable and distributed software solutions that maximize utilization of both local high-performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. This requires a strong background in computer science, providing a platform for technical leadership, but linked to strong personal communication and leadership skills, to help ensure insights are broadly adopted. The successful candidate will be helping us perform research that will transform the lives of cancer patients. Your responsibilities will be to use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be rewsponsible for one major and several minor projects at any point in time. We are in a rapid growth-phase, and the successful candidate will be involved in hiring of new team members. Beyond your strong inter-personal skills and computer science background, you will have experience with either systems software, databases or algorithms, linked to implementation skills at least one of C++, R, Perl or Python. You will be comfortable in UNIX/Linux environments and using continuous integration and CASE tools. Salary Range: $5525-$10925 Monthly
      

        Show more

        


        Show less",Entry level
Diverse Lynx,Jr Data Engineer,"Job DescriptionSkills (Must Have):Python – Ingestion/manipulation of large datasets to S3 using pandasPython – Consumption of data from REST APIs using requestsAny language (Most preferably Python) – Small automation tasks within AWS S3, Glue, AthenaExperience developing in AWS. Key services: S3, Lambda, Athena, Step Functions, GlueGeneral familiarity with a variety of other systems such as Oracle/Postgres, REST APIs, SplunkDeployment of resources to AWS using ServerlessGeneral understand of Infrastructure as CodeSkills (Nice To Have)AI/Client experience in SagemakerGeneral knowledge of cyber security practices and frameworksExperience writing complex queries in Presto/HadoopDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.


        Show more

        


        Show less",Mid-Senior level
Sayari | Commercial Risk Intelligence,Data Engineer,"Sayari is looking for Data Engineers to join our growing team! We are hiring at all levels and encourage junior through senior level candidates to apply. As a member of Sayari's data team you will work with our Product and Software Engineering teams to collect data from around the globe, maintain existing ETL pipelines, and develop new pipelines that power Sayari Graph.About Sayari:Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal & advisory service providers, multinationals, journalists, and governments. We are building world-class SaaS products that help our clients glean insights from vast datasets that we collect, extract, enrich, match and analyze using a highly scalable data pipeline. From financial intelligence to anti-counterfeiting, and from free trade zones to war zones, Sayari powers cross-border and cross-lingual insight into customers, counterparties, and competitors. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering.Our company culture is defined by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, encourage training and learning opportunities, and reward initiative and innovation. If you enjoy working with supportive, high-performing, and curious teams, Sayari is the place for you.Position DescriptionSayari’s flagship product, Sayari Graph, provides instant access to structured business information from hundreds of millions of corporate, legal, and trade records. As a member of Sayari's data team you will work with our Product and Software Engineering teams to collect data from around the globe, maintain existing ETL pipelines, and develop new pipelines that power Sayari Graph.RequirementsWhat You Will Need:Professional experience with Python and a JVM language (e.g., Scala)2+ years of experience designing and maintaining ETL pipelinesExperience using Apache Spark and Apache AirflowExperience with SQL and NoSQL databases (e.g., columns stores, graph, etc.)Experience working on a cloud platform like GCP, AWS, or AzureExperience working collaboratively with gitWhat We Would Like:Understanding of Docker/KubernetesUnderstanding of or interest in knowledge graphsWho You Are:Experienced in supporting and working with cross-functional teams in a dynamic environmentInterested in learning from and mentoring team members Passionate about open source development and innovative technologyBenefitsA collaborative and positive culture - your team will be as smart and driven as youLimitless growth and learning opportunities A strong commitment to diversity, equity, and inclusion Performance and incentive bonuses Outstanding competitive compensation and comprehensive family-friendly benefits, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and parental leave.Conference & Continuing Education Coverage Team building events & opportunitiesSayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply.


        Show more

        


        Show less",Entry level
Ryan,Data Engineer I,"The Data Engineer (DE) position requires a creative problem solver who is passionate about client service. This role will work with the Project Manager and Senior Data Engineers to produce timely, best-in-class deliverables. This role specializes in obtaining, reconciling, analyzing, and preparing data for use in performing sales tax consulting engagements with emphasis on preparing audit ready data populations for the Service Delivery teams quickly and efficiently.PeopleDuties and Responsibilities:Creates a positive team member experience.Demonstrates strong written and verbal communication skills, displays a positive demeanor and team spirit.Plays a key role in driving collaboration across team members, other practices and outside entities.ClientAssists senior team members in retrieving data from client systems.Assists clients and Ryan consultants in data analysis and manipulation.Travels to client sites to assist client in gathering additional data and other necessary documentation.Collaborates with remote and local teammates to ensure efficient design and timely completion of deliverables.Assist senior team members in preparing client and project correspondence.ValueAssists in the acquisition, extraction, and transfer of client data.Analyzes data from client accounting systems to verify accuracy and completeness.Develops and deploys data extraction technologies to perform data extractions from client systems.Manipulates data using Microsoft® Access, SQL, and proprietary software.Assists with the installation of data extraction tools such as Ryan eExtract®, for various ERP systems.Analyzes large data at terabyte scale using modern database technologies.Develops code in Java or Python to support ETL process and contribute to existing code bases. Performs other duties as assigned.Education And ExperienceBS or MBA preferred in information systems or computer science. Other STEM degrees with relevant work experience or coursework are considered.1-3 years of full-time work experience is a plus. Client facing or consulting experience is considered a differentiator.Experience with an enterprise or NoSQL database is a plus.Implementation experience with a major ERP system is a plus.Computer SkillsThe candidate must have a strong command of SQL and uses ETL software such as Visual Studio to build and execute SSIS packages for data manipulation, loading, and processing as well as either Java or Python to perform successfully. Ability to work in the Microsoft Office suite is required.Certificates And LicensesValid driver’s license required.Supervisory ResponsibilitiesThe position requires no direct supervisory responsibilities.Work EnvironmentStandard indoor working environment.Occasional long periods of sitting while working at computer.Position requires regular interaction with employees and clients both in person and via e-mail and telephone.Independent travel requirement: 30 to 40%. Compensation For certain California based roles, the base salary hiring range for this position is $72,069 - $88,044For other California based locations, the base salary hiring range for this position is $66,033 - $80,707For Colorado based roles, the base salary hiring range for this position is $63,032 - $77,039For New York based roles, the base salary hiring range for this position is $72,036 - $88,044For Washington based roles, the base salary hiring range for this position is $66,033 - $80,707The Company makes offers based on many factors, including qualifications and experience.Equal Opportunity Employer: disability/veteran


        Show more

        


        Show less",Entry level
RumbleOn,Data Engineer,"RumbleOn, the nation's largest retailer of powersports vehicles, is adding a Data Engineer to our Software Architecture team to work ON-SITE at our office in Irving, TX. We are seeking a driven, analytical, hands-on Data Engineer who will be responsible for helping to develop the vision for our database architecture and to assist with the creation of a comprehensive data strategy and roadmap, including recommendations for data storage, data integration, data quality, and data models.You will be responsible for organizing and managing cloud data storage, defining appropriate data stores for structured and unstructured data, designing databases for transactional and analytical workloads, and developing pipelines for data transformation.This is a unique opportunity to join a promising company fueled by excitement, innovative ideas, a fun product, and an impressive roadmap for future opportunity. There has never been a better time to join RumbleOn!Responsibilities:Oversee transactional database design in support of applications developmentCollaborate with DBA staff on management of data warehouse in support of analytical workloads Design and build data collection and transformation pipelines Build optimal ETL infrastructure for a variety of data sources Perform validation procedures to ensure data qualityRecommend and implement ways to improve data reliability, efficiency, and qualityDefine security and backup proceduresDevelop and maintains schema designs, data models, data dictionaries, API, and troubleshooting documentationRequirements5+ years software development experience3+ years experience in Data EngineeringDegree in Math, Statistics, Computer Science or a related degree or equivalent experience Certified Data Engineer or equivalent experienceProficiency with SQL, Python, and JavaProficiency with AWS cloud storage offerings for all data types and workloads (Dynamo, Redshift, Snowflake)Experience with big data tools and pipelines, including stream processing (Airflow, Kinesis, DBT)Experience with data pipelines and workflow management tools (Pipeline, Glue)Experience managing cloud services for OLAP, document, and NoSQL databasesStrong written and verbal communication skillsExposure to analytical tools (Athena, Power BI) Strongly DesiredBenefitsWhat RumbleOn Offers You:A fun, relaxed, and casual work environment with awesome people by your side working as a team to ensure the entire group's success! Plus...Healthcare, Dental, & Vision Insurance (RumbleOn pays a generous portion of employee's medical premium!)Generous Vacation/PTO PlanClose knit, open, inviting startup environment where you can make your mark and where your ideas are heard!Employee discounts on purchasesExtremely competitive compensation packages commensurate with experience and skillsetFully stocked kitchen with drinks and snacks all dayFun company eventsThe opportunity for growth and a solid long term career...we promote from within!!Casual Dress codeTraining and full support while you learnAnd more…All applicants must pass pre-employment testing to include: background checks, MVR, and drug testing in order to qualify for employment*


        Show more

        


        Show less",Associate
Massachusetts Institute of Technology,Software and Data Junior Engineer,"Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.


        Show more

        


        Show less",Mid-Senior level
Oshi Health,Data Engineer,"Do you love to work with data, finding ways to make it reportable, and building models that will add clinical and commercial value for the future?Do you want to bring your skills and experience to a growth stage engineering team, and help set us up for smart expansion?Are you excited by the prospect of having a high-visibility high-impact role in a fast-moving startup?Are you passionate about healthcare, and looking to create a revolutionary new approach to digestive healthcare with a radically better patient experience?If so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company.Oshi Health is revolutionizing GI care with a virtual clinic that provides easy, convenient access to a multidisciplinary care team including a GI Physician, Registered Dietician, Mental Health Professional, and Health Coach that takes a whole-person approach to diagnosing, managing and treating digestive health conditions. Our care is built on the latest evidence-based protocols and is delivered virtually through an app, secure messaging and telehealth visits with the care team. NOTE: Oshi is a fully remote company, with team members all over the US.What You’ll Do:This role will be a perfect fit if you enjoy learning the entire stack and taking on interdisciplinary challenges. A primary focus will be building out a data engineering program, including ETL, data governance, sanitization, and data operations. This part of your responsibilities will be a balance of executing data operations, and building automation to make those operations scalable.You will also have the opportunity to join engineers on the frontend end (React Native and React.js), backend (Node.js Lambdas) and Salesforce to help build the Oshi platform. Experience with any of these technologies is a plus, but more important is an enthusiasm to adapt and learn the ones that are new to you.What you’ll do: build the Oshi data programImplement and maintain data pipelines using Stitch, Databricks, and other scripting as needed to feed a PostgreSQL schema supporting Tableau reportingSupport users of Tableau by updating data sources or modifying inbound inputs as needed deliver critical BI reportsOwn the Oshi data model, ensuring that new features built and new technologies adopted serve the needs of the clinical, commercial, product, and engineering teamsManage ETL of client eligibility files and other data, to make them available for Oshi use in a secure and timely manner. Wherever possible, replace bespoke processes with automationOnce you have a understanding of Oshi’s requirements, design and implement a data strategy, (with your recommendation of approach and products,) to meet the needs of Oshi’s analytics, commercial, and clinical business linesYour work will also include:AWS maintenance and administration Writing technical documentation to outline designs for forthcoming features, outlining the implementation across all technology layersMeeting with colleagues in Strategy, Product, and Clinical to support their needs from the Engineering group.Production support responsibilities (shared with the entire engineering team) responding to alerts in Datadog, reviewing and troubleshooting issuesOur tech stack:Mobile Platforms Supported: iOS & AndroidCross-Platform Mobile Language: React NativeOther Languages: React-js, HTML, CSS, Java (Salesforce Apex), Node.js (Lambda)Systems: Salesforce, AWS Amplify / Cognito / LambdaYour Profile:A minimum of 3+ years of professional experienceBachelor's Degree or equivalent experienceGood interpersonal and relationship skills that include a positive attitudeSelf-starter who can find a way forward even when the path is unclear.Team player AND a leader simultaneously.What You’ll Bring to the Team:Passionate about creating value that changes people's livesMake low-level decisions quickly while being patient and methodical with high-level onesAre curious and passionate about digging into new technologies with a knack for picking them up quicklyAdept at prioritizing value and shipping complex products while coordinating across multiple teamsLove working with a diverse set of engineers, product managers, designers, and business partnersStrive to excel, innovate and take pride in your workWork well with other leadersAre a positive culture driverExcited about working in a fast-paced, startup cultureExperience in a regulated industry (healthcare, finance, etc.) a plusand perks:We’re revolutionizing GI care — and our employees are driving the change. We’re a hard-working and fun-loving team, committed to always learning and improving, and dedicated to doing the right thing for our members. To achieve our mission, we invest in our people:We make healthcare more equitable and accessible:Mission-driven organization focused on innovative digestive careThrive on diversity with monthly DEIB discussions, activities, and moreVirtual-first culture: Work from home anywhere in the USLive our core values: Own the outcome, Do the right thing, Be direct and open, Learn and improve, Team, Thrive on diversity We take care of our people:Competitive compensation and meaningful equityEmployer-sponsored medical, dental and vision plans Access to a “Life Concierge” through Overalls, because we know life happensTailored professional development opportunities to learn and growWe rest, recharge and re-energize:Unlimited paid time off — take what you need, when you need it13 paid company holidays to power downTeam events, such as virtual cooking classes, games, and moreRecognition of professional and personal accomplishmentsOshi Health’s Core Values:Go For ItDo the Right ThingBe Direct & OpenLearn & ImproveTEAM - Together Everyone Achieves MoreOshi Health is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Powered by JazzHRFRVWJuzRKn
      

        Show more

        


        Show less",Mid-Senior level
BEACON STREET STUDIOS INC,Jr Data Engineer,"Systems Planning and Analysis, Inc. (SPA) delivers high-impact, technical solutions to complex national security issues. With over 50 years of business expertise and consistent growth, we are known for continuous innovation for our government customers, in both the US and abroad. Our exceptionally talented team is highly collaborative in spirit and practice, producing Results that Matter. Come work with the best! We offer opportunity, unique challenges, and clear-sighted commitment to the mission. SPA: Objective. Responsive. Trusted.SPA assists the Defense Threat Reduction Agency (DTRA) in developing leading edge technologies to counter WMD in support of CCMDs and transitioning these technologies to services and SOCOM. SPA provides expertise across the full range of chemical, biological, radiological, nuclear and high-yield explosives (CBRNE) WMD, Counter Improvised Threat (CIT) and Countering Threat Network (CTN) technologies to support understanding, detection, identification, characterization, denial, control, disabling, defeating, disposing, safeguarding the force, managing consequences, and test and evaluation in support of military and civilian operations.SPA has a near term need for a Jr. Data Engineer. The qualified candidate will provide Advisory and Assistance Support (A&AS) to the Defense Threat Reduction Agency (DTRA), Chemical and Biological Technologies Department, Digital Battlespace Management Division (RD-CBI). The candidate will provide subject matter expertise and support to the client for science and technology (S&T) projects aimed at developing software tools for Chemical, Biological, Radiological, Nuclear (CBRN) CBRN Support to Command and Control (CSC2) and other chemical and biological defense applications.The work is located in Lorton, VA.Travel is expected (~10%).Minimum QualificationsBachelor's degree in Operations Research, Mathematics, Chemical Sciences, or Biological Sciences with 2 to 5 years of experience.Building scalable and dependable Cloud-based and hybrid data solutions by leverage data models, process maps, ETL and data integration processes.Using Python, Java, or Scala Development to meet data quality and data management needs.Using both relational and NOSQL paradigms for data centric solutions.The design and development of data pipelines, enterprise data warehouse, such as Hadoop/Spark in a complex ecosystem.Active DoD Secret Clearance and the ability to maintain throughout time of employment.Preferred QualificationsExperience advising on all phases of the data science problem life-cycle, including use-case identification and formulation, stakeholder communication and management, and placing models into production.Experience advising / advocating for compute architecture decisions, including tools required for projects, ETL pipelines, and data storage and engineering topics as they influence data science activities.Master’s degree in Operations Research, Mathematics, Chemical Sciences, or Biological Sciences.Proven experience analyzing data to produce reports and recommendations on improving processes efficiency for US Military Projects.Knowledge of and/or experience with DTRA or Chemical/Biological Defense Program.


        Show more

        


        Show less",Mid-Senior level
Johnson & Johnson,Data Engineer Summer Intern,"Job DescriptionAt Johnson & Johnson, we use technology and the power of teamwork to discover new ways to prevent and overcome the world’s the most significant healthcare challenges. Our Corporate, Consumer Health, Medical Devices, and Pharmaceutical teams leverage data, real-world insights, and creative minds to make life-changing healthcare products and medicines. We're disrupting outdated healthcare ecosystems and infusing them with transformative ideas to help people thrive throughout every stage of their lives. With a reach of more than a billion people every day, there’s no limit to the impact you can make here. Are you ready to reimagine healthcare?Here, your career breakthroughs will change the future of health, in all the best ways. And you’ll change, too. You’ll be inspired, and you’ll inspire people across the world to change how they care for themselves and those they love. Amplify your impact. Join us! Tasked with transforming data into a format that can be easily consumes by ML algorithms. Adept Database basics SQL, capable of basic data engineering techniques – data wrangling, data cleansing, data curation. Fluent in one of the programming languages R or Python.  Data Engineering skills, e.g., cleaning/organizing data, data preparation, data collection, data integration across different data sources, data storage, relational/non-relational database management.  Proficient in structured query language (SQL), NoSQL, MATLAB, and programming in Python or R.  Proficient in Applied Machine Learning – Experience with a variety of algorithms, Building Predictive Models, Cross-validating, Hypertuning and Deployment.  Experience with devising and implementing ML methods for protein or antibody modeling or design, or with deep learning methods that relate protein sequence, structure, property or function.  MD simulation software (e.g., Commercial tools like Schrodinger, or open-source software AMBER, GROMACS, etc.). Additionally, prior experience in homology modeling and structural refinement  Knowledgeable of AWS services including Redshift, RDS, EMR and EC2  Working knowledge using software and tools including big data tools like Kafka, Spark and Hadoop. At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.Primary LocationNA-US-Pennsylvania-Spring HouseOrganizationJanssen Research & Development, LLC (6084)Job FunctionAdministration


        Show more

        


        Show less",Not Applicable
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $90K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insuranceOptional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatmentSchedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
,,,
,,,
,,,
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
,,,
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
,,,
,,,
Diverse Lynx,Jr Data Engineer,"Job DescriptionSkills (Must Have):Python – Ingestion/manipulation of large datasets to S3 using pandasPython – Consumption of data from REST APIs using requestsAny language (Most preferably Python) – Small automation tasks within AWS S3, Glue, AthenaExperience developing in AWS. Key services: S3, Lambda, Athena, Step Functions, GlueGeneral familiarity with a variety of other systems such as Oracle/Postgres, REST APIs, SplunkDeployment of resources to AWS using ServerlessGeneral understand of Infrastructure as CodeSkills (Nice To Have)AI/Client experience in SagemakerGeneral knowledge of cyber security practices and frameworksExperience writing complex queries in Presto/HadoopDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.


        Show more

        


        Show less",Mid-Senior level
Sayari | Commercial Risk Intelligence,Data Engineer,"Sayari is looking for Data Engineers to join our growing team! We are hiring at all levels and encourage junior through senior level candidates to apply. As a member of Sayari's data team you will work with our Product and Software Engineering teams to collect data from around the globe, maintain existing ETL pipelines, and develop new pipelines that power Sayari Graph.About Sayari:Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal & advisory service providers, multinationals, journalists, and governments. We are building world-class SaaS products that help our clients glean insights from vast datasets that we collect, extract, enrich, match and analyze using a highly scalable data pipeline. From financial intelligence to anti-counterfeiting, and from free trade zones to war zones, Sayari powers cross-border and cross-lingual insight into customers, counterparties, and competitors. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering.Our company culture is defined by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, encourage training and learning opportunities, and reward initiative and innovation. If you enjoy working with supportive, high-performing, and curious teams, Sayari is the place for you.Position DescriptionSayari’s flagship product, Sayari Graph, provides instant access to structured business information from hundreds of millions of corporate, legal, and trade records. As a member of Sayari's data team you will work with our Product and Software Engineering teams to collect data from around the globe, maintain existing ETL pipelines, and develop new pipelines that power Sayari Graph.RequirementsWhat You Will Need:Professional experience with Python and a JVM language (e.g., Scala)2+ years of experience designing and maintaining ETL pipelinesExperience using Apache Spark and Apache AirflowExperience with SQL and NoSQL databases (e.g., columns stores, graph, etc.)Experience working on a cloud platform like GCP, AWS, or AzureExperience working collaboratively with gitWhat We Would Like:Understanding of Docker/KubernetesUnderstanding of or interest in knowledge graphsWho You Are:Experienced in supporting and working with cross-functional teams in a dynamic environmentInterested in learning from and mentoring team members Passionate about open source development and innovative technologyBenefitsA collaborative and positive culture - your team will be as smart and driven as youLimitless growth and learning opportunities A strong commitment to diversity, equity, and inclusion Performance and incentive bonuses Outstanding competitive compensation and comprehensive family-friendly benefits, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and parental leave.Conference & Continuing Education Coverage Team building events & opportunitiesSayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply.


        Show more

        


        Show less",Entry level
Ryan,Data Engineer I,"The Data Engineer (DE) position requires a creative problem solver who is passionate about client service. This role will work with the Project Manager and Senior Data Engineers to produce timely, best-in-class deliverables. This role specializes in obtaining, reconciling, analyzing, and preparing data for use in performing sales tax consulting engagements with emphasis on preparing audit ready data populations for the Service Delivery teams quickly and efficiently.PeopleDuties and Responsibilities:Creates a positive team member experience.Demonstrates strong written and verbal communication skills, displays a positive demeanor and team spirit.Plays a key role in driving collaboration across team members, other practices and outside entities.ClientAssists senior team members in retrieving data from client systems.Assists clients and Ryan consultants in data analysis and manipulation.Travels to client sites to assist client in gathering additional data and other necessary documentation.Collaborates with remote and local teammates to ensure efficient design and timely completion of deliverables.Assist senior team members in preparing client and project correspondence.ValueAssists in the acquisition, extraction, and transfer of client data.Analyzes data from client accounting systems to verify accuracy and completeness.Develops and deploys data extraction technologies to perform data extractions from client systems.Manipulates data using Microsoft® Access, SQL, and proprietary software.Assists with the installation of data extraction tools such as Ryan eExtract®, for various ERP systems.Analyzes large data at terabyte scale using modern database technologies.Develops code in Java or Python to support ETL process and contribute to existing code bases. Performs other duties as assigned.Education And ExperienceBS or MBA preferred in information systems or computer science. Other STEM degrees with relevant work experience or coursework are considered.1-3 years of full-time work experience is a plus. Client facing or consulting experience is considered a differentiator.Experience with an enterprise or NoSQL database is a plus.Implementation experience with a major ERP system is a plus.Computer SkillsThe candidate must have a strong command of SQL and uses ETL software such as Visual Studio to build and execute SSIS packages for data manipulation, loading, and processing as well as either Java or Python to perform successfully. Ability to work in the Microsoft Office suite is required.Certificates And LicensesValid driver’s license required.Supervisory ResponsibilitiesThe position requires no direct supervisory responsibilities.Work EnvironmentStandard indoor working environment.Occasional long periods of sitting while working at computer.Position requires regular interaction with employees and clients both in person and via e-mail and telephone.Independent travel requirement: 30 to 40%. Compensation For certain California based roles, the base salary hiring range for this position is $72,069 - $88,044For other California based locations, the base salary hiring range for this position is $66,033 - $80,707For Colorado based roles, the base salary hiring range for this position is $63,032 - $77,039For New York based roles, the base salary hiring range for this position is $72,036 - $88,044For Washington based roles, the base salary hiring range for this position is $66,033 - $80,707The Company makes offers based on many factors, including qualifications and experience.Equal Opportunity Employer: disability/veteran


        Show more

        


        Show less",Entry level
Oshi Health,Data Engineer,"Do you love to work with data, finding ways to make it reportable, and building models that will add clinical and commercial value for the future?Do you want to bring your skills and experience to a growth stage engineering team, and help set us up for smart expansion?Are you excited by the prospect of having a high-visibility high-impact role in a fast-moving startup?Are you passionate about healthcare, and looking to create a revolutionary new approach to digestive healthcare with a radically better patient experience?If so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company.Oshi Health is revolutionizing GI care with a virtual clinic that provides easy, convenient access to a multidisciplinary care team including a GI Physician, Registered Dietician, Mental Health Professional, and Health Coach that takes a whole-person approach to diagnosing, managing and treating digestive health conditions. Our care is built on the latest evidence-based protocols and is delivered virtually through an app, secure messaging and telehealth visits with the care team. NOTE: Oshi is a fully remote company, with team members all over the US.What You’ll Do:This role will be a perfect fit if you enjoy learning the entire stack and taking on interdisciplinary challenges. A primary focus will be building out a data engineering program, including ETL, data governance, sanitization, and data operations. This part of your responsibilities will be a balance of executing data operations, and building automation to make those operations scalable.You will also have the opportunity to join engineers on the frontend end (React Native and React.js), backend (Node.js Lambdas) and Salesforce to help build the Oshi platform. Experience with any of these technologies is a plus, but more important is an enthusiasm to adapt and learn the ones that are new to you.What you’ll do: build the Oshi data programImplement and maintain data pipelines using Stitch, Databricks, and other scripting as needed to feed a PostgreSQL schema supporting Tableau reportingSupport users of Tableau by updating data sources or modifying inbound inputs as needed deliver critical BI reportsOwn the Oshi data model, ensuring that new features built and new technologies adopted serve the needs of the clinical, commercial, product, and engineering teamsManage ETL of client eligibility files and other data, to make them available for Oshi use in a secure and timely manner. Wherever possible, replace bespoke processes with automationOnce you have a understanding of Oshi’s requirements, design and implement a data strategy, (with your recommendation of approach and products,) to meet the needs of Oshi’s analytics, commercial, and clinical business linesYour work will also include:AWS maintenance and administration Writing technical documentation to outline designs for forthcoming features, outlining the implementation across all technology layersMeeting with colleagues in Strategy, Product, and Clinical to support their needs from the Engineering group.Production support responsibilities (shared with the entire engineering team) responding to alerts in Datadog, reviewing and troubleshooting issuesOur tech stack:Mobile Platforms Supported: iOS & AndroidCross-Platform Mobile Language: React NativeOther Languages: React-js, HTML, CSS, Java (Salesforce Apex), Node.js (Lambda)Systems: Salesforce, AWS Amplify / Cognito / LambdaYour Profile:A minimum of 3+ years of professional experienceBachelor's Degree or equivalent experienceGood interpersonal and relationship skills that include a positive attitudeSelf-starter who can find a way forward even when the path is unclear.Team player AND a leader simultaneously.What You’ll Bring to the Team:Passionate about creating value that changes people's livesMake low-level decisions quickly while being patient and methodical with high-level onesAre curious and passionate about digging into new technologies with a knack for picking them up quicklyAdept at prioritizing value and shipping complex products while coordinating across multiple teamsLove working with a diverse set of engineers, product managers, designers, and business partnersStrive to excel, innovate and take pride in your workWork well with other leadersAre a positive culture driverExcited about working in a fast-paced, startup cultureExperience in a regulated industry (healthcare, finance, etc.) a plusand perks:We’re revolutionizing GI care — and our employees are driving the change. We’re a hard-working and fun-loving team, committed to always learning and improving, and dedicated to doing the right thing for our members. To achieve our mission, we invest in our people:We make healthcare more equitable and accessible:Mission-driven organization focused on innovative digestive careThrive on diversity with monthly DEIB discussions, activities, and moreVirtual-first culture: Work from home anywhere in the USLive our core values: Own the outcome, Do the right thing, Be direct and open, Learn and improve, Team, Thrive on diversity We take care of our people:Competitive compensation and meaningful equityEmployer-sponsored medical, dental and vision plans Access to a “Life Concierge” through Overalls, because we know life happensTailored professional development opportunities to learn and growWe rest, recharge and re-energize:Unlimited paid time off — take what you need, when you need it13 paid company holidays to power downTeam events, such as virtual cooking classes, games, and moreRecognition of professional and personal accomplishmentsOshi Health’s Core Values:Go For ItDo the Right ThingBe Direct & OpenLearn & ImproveTEAM - Together Everyone Achieves MoreOshi Health is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Powered by JazzHRFRVWJuzRKn
      

        Show more

        


        Show less",Mid-Senior level
BEACON STREET STUDIOS INC,Jr Data Engineer,"Systems Planning and Analysis, Inc. (SPA) delivers high-impact, technical solutions to complex national security issues. With over 50 years of business expertise and consistent growth, we are known for continuous innovation for our government customers, in both the US and abroad. Our exceptionally talented team is highly collaborative in spirit and practice, producing Results that Matter. Come work with the best! We offer opportunity, unique challenges, and clear-sighted commitment to the mission. SPA: Objective. Responsive. Trusted.SPA assists the Defense Threat Reduction Agency (DTRA) in developing leading edge technologies to counter WMD in support of CCMDs and transitioning these technologies to services and SOCOM. SPA provides expertise across the full range of chemical, biological, radiological, nuclear and high-yield explosives (CBRNE) WMD, Counter Improvised Threat (CIT) and Countering Threat Network (CTN) technologies to support understanding, detection, identification, characterization, denial, control, disabling, defeating, disposing, safeguarding the force, managing consequences, and test and evaluation in support of military and civilian operations.SPA has a near term need for a Jr. Data Engineer. The qualified candidate will provide Advisory and Assistance Support (A&AS) to the Defense Threat Reduction Agency (DTRA), Chemical and Biological Technologies Department, Digital Battlespace Management Division (RD-CBI). The candidate will provide subject matter expertise and support to the client for science and technology (S&T) projects aimed at developing software tools for Chemical, Biological, Radiological, Nuclear (CBRN) CBRN Support to Command and Control (CSC2) and other chemical and biological defense applications.The work is located in Lorton, VA.Travel is expected (~10%).Minimum QualificationsBachelor's degree in Operations Research, Mathematics, Chemical Sciences, or Biological Sciences with 2 to 5 years of experience.Building scalable and dependable Cloud-based and hybrid data solutions by leverage data models, process maps, ETL and data integration processes.Using Python, Java, or Scala Development to meet data quality and data management needs.Using both relational and NOSQL paradigms for data centric solutions.The design and development of data pipelines, enterprise data warehouse, such as Hadoop/Spark in a complex ecosystem.Active DoD Secret Clearance and the ability to maintain throughout time of employment.Preferred QualificationsExperience advising on all phases of the data science problem life-cycle, including use-case identification and formulation, stakeholder communication and management, and placing models into production.Experience advising / advocating for compute architecture decisions, including tools required for projects, ETL pipelines, and data storage and engineering topics as they influence data science activities.Master’s degree in Operations Research, Mathematics, Chemical Sciences, or Biological Sciences.Proven experience analyzing data to produce reports and recommendations on improving processes efficiency for US Military Projects.Knowledge of and/or experience with DTRA or Chemical/Biological Defense Program.


        Show more

        


        Show less",Mid-Senior level
,,,
Massachusetts Institute of Technology,Data Engineer,"Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.


        Show more

        


        Show less",Entry level
Bloom Insurance,Data Engineer I,"To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.Essential FunctionsDay to day functions include the following:Design data models and develop database structures in Microsoft SQL server.Write various database objects like stored procedures, functions, views, triggers for various front end applications.Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.Create database deployment packages for deploying changes.Identify & repair inconsistencies in data, database tuning, query optimization.Able to generate ad hoc data on demand.Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise mannerDevelop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.DocumentationOptimization recommendationsDay to day troubleshooting.NET Programming as neededEducation/ExperienceBA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experienceSolid experience with various versions of MS SQL Server and TSQL programmingMicrosoft Certified DBA a plusSkills/KnowledgeStrong experience in writing efficient SQL codeWorking knowledge of SQL Server Management Studio (SSMS)Knowledge of SQL Server Reporting Services (SSRS)Knowledge of SQL Server Integration Services (SSIS)Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plusKnowledge of data science technologies is a plusClear, concise communication skills, excellent organizational skillsHighly self-motivated and directedKeen attention to detailHigh level of work intensity in a team environmentHigh integrity and values-drivenEager for professional developmentExperience and understanding of source control management a plusWhat We OfferAt Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:Competitive compensation Comprehensive health benefits Long-term career growth and mentoring About BloomAs an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.Ascend Technology ™Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
      

        Show more

        


        Show less",Entry level
PepsiCo,Data Engineer,"OverviewPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.What PepsiCo Data Management and Operations does: Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders Increase awareness about available data and democratize access to it across the companyJob DescriptionAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, onpremise data sources as well as cloud and remote systems.ResponsibilitiesActive contributor to code development in projects and services.Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.Responsible for implementing best practices around systems integration, security, performance and data management.Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.Develop and optimize procedures to “productionalize” data science models.Define and manage SLA’s for data products and processes running in production.Support large-scale experimentation done by data scientists.Prototype new approaches and build solutions at scale.Research in state-of-the-art methodologies.Create documentation for learnings and knowledge transfer.Create and audit reusable packages or libraries.Qualifications4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).2+ years in cloud data engineering experience in Azure.Fluent with Azure cloud services. Azure Certification is a plus.Experience with integration of multi cloud services with on-premises technologies.Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.Experience with version control systems like Github and deployment & CI tools.Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools is a plus.Experience with Statistical/ML techniques is a plus.Experience with building solutions in the retail or in the supply chain space is a plusUnderstanding of metadata management, data lineage, and data glossaries is a plus.Working knowledge of agile development, including DevOps and DataOps concepts. Familiarity with business intelligence tools (such as PowerBI).EducationBA/BS in Computer Science, Math, Physics, or other technical fields.Skills, Abilities, KnowledgeExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.Proven track record of leading, mentoring data teams.Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.Ability to understand and translate business requirements into data and technical requirements. High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.Foster a team culture of accountability, communication, and self-management.Proactively drives impact and engagement while bringing others along.Consistently attain/exceed individual and team goals.Ability to lead others without direct authority in a matrixed environment.CompetenciesHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.Understands both the engineering and business side of the Data Products released.Places the user in the center of decision making.Teams up and collaborates for speed, agility, and innovation.Experience with and embraces agile methodologies.Strong negotiation and decision-making skill.Experience managing and working with globally distributed teamsCOVID-19 vaccination is a condition of employment for this role. Please note that all such company vaccine requirements provide the opportunity to request an approved accommodation or exemption under applicable lawEEO StatementAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender IdentityIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.Please view our Pay Transparency Statement
      

        Show more

        


        Show less",Mid-Senior level
LogixHealth,Data Engineer,"Location:Bedford, MA; HybridThis Role:As a Data Engineer at LogixHealth, you will work with a globally distributed team of engineers to design, build and maintain cutting edge solutions that will directly improve the healthcare industry. You’ll contribute to our fast-paced, collaborative environment and will bring your expertise to continue delivering innovative technology solutions. The ideal candidate will be able to develop large scale, distributed data pipelines with an eye towards data security, availability and quality. The candidate should be experienced with modern data storage & transmission techniques, big data tools and distributed processes. The candidate should have excellent interpersonal communication and an aptitude to continue learning.Key Responsibilities:Design highly scalable, available and fault tolerant data processing systemsBuild out data quality proceduresCollaborate with other engineers on existing software and data integration solutionsKeep up with the latest technology industry trends and innovations Help other team members learn and adopt new technologies and practicesQuickly learn new and existing technologiesQualifications:To perform this job successfully, an individual must be able to perform each Key Responsibility satisfactorily. The following requirements are representative of the knowledge, skills, and/or ability required to perform this job successfully. Reasonable accommodation may be made to enable individuals with disabilities to perform the duties. Required:BS/MS in Computer Science, related technical field or equivalent experienceStrong programming and scripting skillsData modeling and data warehousing/lakesREST API servicesExpertise in data storage systems, including SQL & NoSQL database systems & file object storageAdvanced SQL and query performance tuning skillsUnderstanding of cloud computing technologies & platformsExperience with gitExcellent interpersonal communication skillsPreferred:Big data analysis techniquesBig data visualization solutionsDistributed systems design Healthcare industry knowledgeBenefits at LogixHealth:We offer a comprehensive benefits package including health, dental and vision, 401(k), PTO, paid holidays, life and disability insurance, on-site fitness center and company-wide social events.About LogixHealth:At LogixHealth we provide expert coding and billing services that allow physicians to focus on providing great clinical care. LogixHealth was founded in the 1990s by physicians to service their own practices and has grown to become the nation’s leading provider of unsurpassed software-enabled revenue cycle management services, offering a complete range of solutions, including coding and claims management and the latest business intelligence reporting dashboards for clients in 40 states.Since our first day, we have had a clear vision of a better healthcare system and have continually evolved to get there. In addition to providing expert revenue cycle services, we utilize proprietary software to provide valuable financial, clinical, and other data insights that directly improve the quality and efficiency of patient care.At LogixHealth, we’re committed to Making intelligence matter through our pillars of Physician-Inspired Knowledge, Unrivaled Technology and Impeccable Service.To learn more about us, visit our website https://www.logixhealth.com/.


        Show more

        


        Show less",Entry level
Johnson & Johnson,Data Engineer Summer Intern,"Job DescriptionAt Johnson & Johnson, we use technology and the power of teamwork to discover new ways to prevent and overcome the world’s the most significant healthcare challenges. Our Corporate, Consumer Health, Medical Devices, and Pharmaceutical teams leverage data, real-world insights, and creative minds to make life-changing healthcare products and medicines. We're disrupting outdated healthcare ecosystems and infusing them with transformative ideas to help people thrive throughout every stage of their lives. With a reach of more than a billion people every day, there’s no limit to the impact you can make here. Are you ready to reimagine healthcare?Here, your career breakthroughs will change the future of health, in all the best ways. And you’ll change, too. You’ll be inspired, and you’ll inspire people across the world to change how they care for themselves and those they love. Amplify your impact. Join us! Tasked with transforming data into a format that can be easily consumes by ML algorithms. Adept Database basics SQL, capable of basic data engineering techniques – data wrangling, data cleansing, data curation. Fluent in one of the programming languages R or Python.  Data Engineering skills, e.g., cleaning/organizing data, data preparation, data collection, data integration across different data sources, data storage, relational/non-relational database management.  Proficient in structured query language (SQL), NoSQL, MATLAB, and programming in Python or R.  Proficient in Applied Machine Learning – Experience with a variety of algorithms, Building Predictive Models, Cross-validating, Hypertuning and Deployment.  Experience with devising and implementing ML methods for protein or antibody modeling or design, or with deep learning methods that relate protein sequence, structure, property or function.  MD simulation software (e.g., Commercial tools like Schrodinger, or open-source software AMBER, GROMACS, etc.). Additionally, prior experience in homology modeling and structural refinement  Knowledgeable of AWS services including Redshift, RDS, EMR and EC2  Working knowledge using software and tools including big data tools like Kafka, Spark and Hadoop. At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.Primary LocationNA-US-Pennsylvania-Spring HouseOrganizationJanssen Research & Development, LLC (6084)Job FunctionAdministration


        Show more

        


        Show less",Not Applicable
Pura,Data Engineer (Mid/Jr),"Data Engineer (Mid/Jr Level)Pura has been revolutionizing the smart home experience for the past several years. We obsess over providing world-class experiences for our customers, partners, and vendors. We pride ourselves on maintaining a high standard of quality and innovation with our products, and continuous growth and development for our people.We are looking for a Data Engineer to help business users and analysts throughout the organization access the data they need to operate and grow the business.What you’ll own:In this high-impact role, you will:Work closely with the Data Science team to design and develop scalable data pipelines for processing and analyzing large volumes of dataBuild and maintain ETL processes using Python, SQL, Apache Airflow, and other technologiesDevelop and deploy data processing jobs on AWS or GCP using Docker and KubernetesWrite API wrappers to integrate with various external data sources and third-party toolsImplement and maintain best practices for data security, data quality, and data governanceCollaborate with other cross-functional teams to ensure data is available, reliable, and accessible to support business decisionsWrite clean, readable, and maintainable code and ensure code is thoroughly tested and documentedQualifications:Bachelor's degree in Computer Science, Software Engineering, or related field1-3 years of experience in data engineering or a related fieldProficiency in Python, SQL, Apache Airflow, and DockerExperience with AWS or GCP and some Kubernetes experienceStrong analytical and problem-solving skillsExcellent communication and collaboration skillsAbility to work independently and as part of a teamPassion for writing clean, readable code and ensuring code qualityIf you are passionate about data engineering and want to join a fast-paced, dynamic team that is making a real impact, we encourage you to apply today!.Pura’s StoryAt Pura, we’re pairing smart tech with premium fragrance to create a perfectly personalized and customized scenting experience for the individual. We partner with brands like Disney, Capri Blue, and Anthropologie to bring original and well-loved fragrances to homes in a modern, convenient, and safe way. We know we’ve only just begun to unlock the possibility of scent, and we’re excited for the opportunities that lie ahead.We’re quickly turning heads and getting noticed. We raised a seed round of 4.4M in February of 2020, was recognized by Inc. Magazine as a 2021 Best Workplace, won the Silicon Slopes Hall of Fame & Awards Advertising category in 2022, and we’re currently the 6th-fastest growing company in Utah. Check out our Instagram @pura and TikTok @trypura channels for a look into the excited, engaged community we’re building. We pride ourselves on being a human brand and in creating a culture worth talking about, and we have big goals for the future.Join the Pura Team!All candidates are subject to a background check.Pura provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Powered by JazzHRPSRu221guI
      

        Show more

        


        Show less",Mid-Senior level
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
Intuit,Data Engineer III (Mailchimp),"OverviewMailchimp is a leading marketing platform for small businesses. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaigns, CRM, and analytics tools.The Data Platform team at Mailchimp is responsible for creating and managing our BI platform that enables peeps from across the company to make better decisions with data. What that actually looks like is working with folks from across the company to understand their needs and then surface data in a meaningful way, across tools and teams, to help them drive the business forward. Day to day you could be building an ETL pipeline, designing a data access tool, modeling out new data in Looker, or working with teams to develop better data governance practices. We have a lot of tools in our toolbelt, but all with the main goal helping Mailchimp to continue to use data more effectively.Intuit Mailchimp is a hybrid workplace , giving employees the opportunity to collaborate in person with team members in our Atlanta and Brooklyn offices two or more days per week.What You'll BringExperience with Python, Java, Go, or another OOP languageExperience with SQL and data analysis skillsExperience with data transformation tools like dbt or DataformExperience in visualization technologies like Looker, Tableau, or Qlik SenseExperience with Airflow or other ETL orchestration toolsSolid business intuition and ability to understand and work with cross-functional partnersExperience with GCP/AWS or other cloud providers is preferredBachelors degree is Computer Science or equivalent experience Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Mailchimp we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.How You Will LeadPartner with analysts, data scientists, business users, and other engineers to understand their needs and come up with data solutionsHelp build robust transformation pipelines to help clean up, normalize, and govern our data for broad consumptionHelp build scalable transformation pipelines that enables teams to easily clean up, normalize, and govern data for broad consumptionBuild tools and processes to help make the right data accessible to the right peopleModel data in Looker, to provide a collaborative data analytics platform for the companyWork with Data Stewards to better document our data


        Show more

        


        Show less",Mid-Senior level
2Bridge Partners,Data Engineer,"Well known NY based Non-Profit seeks a Data Engineer.An ideal candidate will have strong data engineering skills with both SQL and Python.This position can be based out of NYC or be 100% remote.Compensation includes salary, excellent medical, dental, vision benefits, pto, 401k, a quality of life oriented work life balance, and lots of other benefits.ResponsibilitiesThis position will work within the enterprise data engineering team to continuously build, maintain, and improve the design, performance, reliability, scalability, security, and architecture of enterprise data products.Responsibilities include:Develop robust database solutions, data integration (ETL, ELT) packages, automation, performance monitoring, SQL coding, database tuning and optimizations, security management, capacity planning, database HA, and database DR solutions across required data platforms.Conduct in-depth data analysis to support data product design.Identify and resolve production database and data integration issues.Collaborates closely with data quality, application, and visualization teams to deliver high-quality data products.Participate in code review, QA, release, and continuous deployment processes.Qualifications:Bachelors Degree in Computer Science or other quantitative disciplines such as Science, Statistics, Economics or Mathematics.5+ years of progressive database development experience with the Microsoft SQL Server family suite.Experience with data modeling and data architecture principles for both analytics and transactional systems.Hands-on experience with SQL Server, Oracle, or other RDBMS required.Python ScriptingExperience with Cloud Data PlatformsPreferred Experience:Scripting experience in Powershell, C#, Java, and/or R.Experience in the Microsoft Azure technology stack is a plus.Experience with data analytics and visualization is a plus.Preferred Knowledge:Intermediate knowledge of OLTP and OLAP database designIntermediate knowledge of data modeling (normalized and dimensional)Intermediate knowledge of ETL, ELT architecture especially for real and near-real-time scenariosIntermediate knowledge of Data Architecture implementation patternsAnticipated salary for candidates living in the NY Metro market in the 140,000 to 160,000 range.


        Show more

        


        Show less",Mid-Senior level
Laguna Games,Data Engineer,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platformDesign and build scalable data solutions that support the real-time analytics and reporting needs of the businessCollaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetizationBuild and maintain data models, data warehouses and data marts to support business intelligence and reporting needsEnsure data quality, integrity and security across all data sources and systemsMonitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecksKeep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructureQualificationsBachelor's degree in Computer Science, Computer Engineering, or related field3+ years of experience in data engineering or related fieldExperience with big data technologies such as Hadoop, Spark, Kafka, and ElasticsearchStrong proficiency in SQL, Python and/or Java programming languagesExperience with cloud-based data solutions such as AWS, Azure or Google CloudFamiliarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence toolsUnderstanding of blockchain technology and its use in gaming platforms is a plusExcellent communication skills, both written and verbal, and ability to collaborate with cross-functional teamsIf you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.Learn more here!https://laguna.gameshttps://www.cryptounicorns.fun
      

        Show more

        


        Show less",Entry level
Pulivarthi Group (PG),Data Engineer,"Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/Pulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.We’ve served some of the largest healthcare, financial services, and government entities in the U.S.Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/Data Engineer - Experience in data engineering and building applications; Experience in Python/PySpark; Experience in Typescript (Preferred) or JavascriptExperience in building applications or dashboards using no- and low-code tools.Coding Skills: Python complete language proficiency; SQL proficiency in querying language (join types, filtering, aggregation) and data modeling (relationship types, constraints);PySpark basic familiarity (DataFrame operations, PySpark SQL functions) and differences with other DataFrame implementations (Pandas);Typescript experience in TypeScript or Javascript;Application building working with no- and low-code tools to query databases, define variables, filters, cross-filters, responsive front-end and user based applications.Databases familiarity with common relational database models and proprietary instantiations, such as SAP, Salesforce etc.; Git knowledge of version control / collaboration workflows and best practices; Agile familiarity with agile and iterative working methodology and rapid user feedback gathering concepts; UX design knowledge of best practices and applications; Data literacy data analysis and statistical basics to ensure correctness in data aggregation and visualization.
      

        Show more

        


        Show less",Mid-Senior level
,,,
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Geopath,Data Engineer,"Applicants must be authorized to work for any employer in the United States. We are unable to sponsor, or take over sponsorship, of an employment visa at this time.About the roleReporting to the SVP, Data & Technology and Lead Data Engineer, the Data Engineer will play a critical role in supporting and advancing Geopath’s new audience measurement platform.Responsibilities:Build scalable functions, fault tolerant batch & real time data pipelines to validate/extract/transform/integrate large scale datasets inclusive of location and time series data, across multiple platformsReview current data processes to identify improvement areas: design and implement solutions to improve scalability, performance, cost efficiencies and data qualityExtend and optimize Geopath’s data platform, which spans across multiple cloud services, data warehouses, and applications in order to meet the industry’s evolving data needsResearch, evaluate, analyze and integrate new data sources and develop quick prototypes for new data productsWork closely with product owners, data architects, data scientists and application development teams to quickly define prototypes, and build new data productsDevelop a solid understanding of the nuances within Geopath’s foundational data sourcesSupport internal stakeholders including data, design, product and executive teams by assisting them with data-related technical issuesRequirements:Bachelor’s or master's degree in computer science, computer engineering, informatics, or equivalent fields3+ years of experience as a Data Engineer with demonstrated experience in data modeling, ETL, data warehouse/data lakes, and consolidating data from multiple data sources3+ years of experience with SQL, solid experience in writing stored procedures and UDFs, familiarity with Snowflake’s data warehouse a plus2+ years of experience in building and optimizing scalable and robust big data pipelines in Apache Airflow or other workflow engines and fluent in Python or Java programmingGood working knowledge in data partition and query optimizationExperience or desire to work in a start-up environment, good team player, and can work independently with minimal supervisionGreat analytical and problem-solving skills, eager to learn new technologies and willing to wear different hats in a small team settingGreat communication and organizational skillsExpected salary for this role is between $75,000-$140,000 per year, depending on experience.About Geopath Inc.Established in 1933, Geopath, (previously the Traffic Audit Bureau for Media Measurement Inc.,) has been the gold standard in providing media auditing and audience measurement services for the Out of Home industry in the US for 90 years. Geopath has now expanded its historical mission and is looking to the future by modernizing its mission critical, industry-standard media audience rating platform while embracing the digital era. In addition to being the industry’s media auditing solution, Geopath also analyzes people’s movement data, develops deep consumer insights, innovations and advanced media research methodologies to uncover critical insights about how consumers engage with Out of Home advertising across a multi-channel ecosystem and in the physical world.


        Show more

        


        Show less",Mid-Senior level
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
Ford Motor Company,Data Engineer,"At Ford Motor Company, we believe freedom of movement drives human progress. We also believe in providing you with the freedom to define and realize your dreams. With our incredible plans for the future of mobility, we have a wide variety of opportunities for you to accelerate your career potential as you help us define tomorrow's transportation.Ford Motor Company is proud to be an Emerald sponsor of the Grace Hopper Conference, and we are excited to be a part of this event. Whether you are in person in Orlando, or attending virtually, we would love to hear from you!As a Cloud Data Engineer, you will leverage your technical expertise in data, analytics, cloud technologies, and analytic software tools to identify best designs, improve business processes, and generate measurable business outcomes.What You'll Be Able To Do Develop EL/ELT/ETL pipelines to make data available in BigQuery analytical data store from disparate batch, streaming data sources for the Business Intelligence and Analytics teams  Work with on-prem data sources (Hadoop, SQL Server), understand the data model, business rules behind the data and build data pipelines (with GCP, Informatica) for one or more Ford verticals. This data will be landed in GCP BigQuery.  Build cloud-native services and APIs to support and expose data-driven solutions.  Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions.  Design, build and launch shared data services to be leveraged by the internal and external partner developer community.  Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize and monitor data pipelines.  Provide extensive technical, strategic advice and guidance to key stakeholders around data transformation efforts. Understand how data is useful to the enterprise. The Minimum Requirements We Seek Bachelor's Degree in Computer Science, Electrical Engineering or a related field, or a combination of education or equivalent experience.  2+ years of experience with SQL and Python  2+ years of experience with GCP or AWS cloud services; Strong candidates with 5+ years in a traditional data warehouse environment (ETL pipelines with Informatica) will be considered  2+ years of experience building out data pipelines from scratch in a highly distributed and fault-tolerant manner. Our Preferred Qualifications Master's degree in Computer Engineering, Computer Science, or a related field of study.  Experience with GCP cloud services including BigQuery, Cloud Composer, Dataflow, CloudSQL, GCS, Cloud Functions and Pub/Sub.  Inquisitive, proactive, and interested in learning new tools and techniques.  Familiarity with big data and machine learning tools and platforms. Comfortable with open-source technologies including Apache Spark, Hadoop, Kafka.  1+ year experience with Hive, Spark, Scala, JavaScript.  Strong oral, written and interpersonal communication skills  Comfortable working in a dynamic environment where problems are not always well-defined. What you will receive in return: As part of the Ford family, you will enjoy excellent compensation and a comprehensive benefits package that includes generous PTO, retirement, savings and stock investment plans, incentive compensation, and much more. You will also experience exciting opportunities for professional and personal growth and recognition. If you have what it takes to help us redefine the future of mobility, we would love to have you join us. Candidates for positions with Ford Motor Company must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is available for this position.We are an Equal Opportunity Employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status. https://corporate.ford.com/content/dam/corporate/us/en-us/documents/careers/2022-benefits-and-comp-GSR-sal-plan-2.pdfAt Ford, the health and safety of our employees is our top priority. Vaccination has been proven to play a critical role in combating COVID-19. As a result, Ford has made the decision to require U.S. salaried employees to be fully vaccinated against COVID-19, unless employees require an accommodation for religious or medical reasons. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination or an approved medical or religious exemption.About UsFord Motor Company is about more than making world-class vehicles – at Ford we Go Further to make people’s lives better.We do this in every corner of the globe. Ford is both an automotive and mobility company. Across six continents, our employees produce innovative products in our engineering and design centers, research labs and high-tech assembly plants.And in order to do that, we are looking to attract the top talent like you.Ford is a place where development is valued for all, and employees are encouraged to learn, build skills and continuously improve year-after-year. When you work at Ford, you and your team will Go Further each day to deliver great products, build a strong business and contribute to a better world.The distance between you and an amazing career has never been shorter!
      

        Show more

        


        Show less",Entry level
Stellent IT,Python Data Engineer,"Job Title: Python Data Engineer.Location:RemoteModule: Phone+SkypeJob Descriptionneed Only USC and GC.Data & Analytics Development Engineer Primary Role Definition: Data & Analytics developers are people who works with ETL (extract data, transform it, and load it) tools like DataStage, Autosys scheduling tools.Skills RequiredVery skilled in Python and DataStage coding and developmentVery skilled in performing Python/Data Stage Code reviewsVery skilled in SQLVery skilled in AutosysVery skilled in Data Modeling with both Oracle and SQL DatabasesDatabase query tuningCloud experience (AWS/GCP) is a plus.Hit ground running, self-starter, great communicator (verbally and written
      

        Show more

        


        Show less",Entry level
,,,
,,,
RemX - Accounting & Finance Staffing,Data Engineer,"New Opportunity Systems Engineer Greeneville, TN On-site $110-120k plus excellent benefits We are looking for an enthusiastic Systems Engineer to design, develop and install software solutions. The successful candidate will be able to review software applications standards and technical design. Implement new software platforms work with third party vendors. Support and/or install software applications/operating systems. Participate in the testing process through test review and analysis, test witnessing and certification of software. Requires a bachelor's degree in a related area and 2-5years of experience in the field or in a related area. Has knowledge of commonly used concepts, practices, and procedures within a particular field. Rely on instructions and pre-established guidelines to perform the functions of the job. Work as subject manner expert with minimum supervision. Primary job functions will exercise independent judgment when required. Typically reports to a department manager. Responsibilities: * Performance tuning, improvement, balancing, usability, automation * Support, maintain and document software functionality * Integrate new systems with existing systems * Evaluate and identify innovative technologies for implementation * Project planning and Project management * Maintain standards compliance * Implement new software platforms work with third party vendors * Document and demonstrates solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code * Improve operations by conducting systems analysis, recommending changes in policies and procedures * Protect operations by keeping information confidential * Provide information by collecting, analyzing, and summarizing development and service issues * Accomplish engineering and organization mission by completing related results as needed * Develop software/system solutions by studying information needs; conferring with users; studying systems flow, data usage and work processes; investigating problem areas; following the software/system development lifecycle. * Produce specifications and determine operational feasibility * Document and maintain software functionality * Serve as a subject matter expert * Comply with project plans and industry standards * * Requirements: Software Engineer top skills & proficiency: * Proven work experience in software engineering and/or Database Management * Firsthand experience in implementing and maintaining software/system applications * Firsthand experience in Relational Databases, SQL and etc. * Knowledge of ERP Systems * Experience with test-driven development * Ability to document requirements and specifications * Familiarity with software/system development methodology and release processes * Installing and configuring operating systems and application software * BS degree in Computer Science or relevant degree in Information Systems * Proficient in SQL Queries, stored procedures and working with in relational databases like SQL Server, MySQL, and ERP Systems * Analytical & Problem-Solving Skills * Ability to Learn Quickly * Team Player * Project Management * Written and Verbal Communication * Customer-Oriented * Analysis * General Programming Skills * SharePoint, MS Dynamics, HTML, and other related software a PLUS Company Description Each and every day RemX puts over 90,000 people to work, helping more than 15,000 companies find the talent they need in order to succeed. And, as a part of the 10th largest staffing company in the world, we understand that at the heart of every successful business are people. That's why we work hard to find you the right job at the right company. Explore all the exciting opportunities that RemX offers and find the right fit for you!Each and every day RemX puts over 90,000 people to work, helping more than 15,000 companies find the talent they need in order to succeed. And, as a part of the 10th largest staffing company in the world, we understand that at the heart of every successful business are people. That’s why we work hard to find you the right job at the right company. Explore all the exciting opportunities that RemX offers and find the right fit for you!
      

        Show more

        


        Show less",Entry level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Nike,Data Engineer,"Become a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following- Programming ability (Python, SQL);  Database related concept;  Big Data exposure;  Spark;  Airflow (Orchestration tools);  Cloud Solutions;  Software/Data design ability;  CI/CD understanding and implementation;  Code review;  Data Architecture; and  AWS, Azure. NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.]]>


        Show more

        


        Show less",Entry level
Yakoa,Data Engineer,"We're looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoa's product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.ResponsibilitiesDesign, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed (cloud) structures and other applicable storage forms such as vector databases and relational databases.Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure.Design and optimize data storage schemas to make terabytes of data readily accessible to our API.Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns.Utilize and advance continuous integration and deployment frameworks.Research, evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing.Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects.Requirements4+ years working as a data engineer.Proficient in database schema design, and analytical and operational data modeling.Proven experience working with large datasets and big data ecosystems for computing (spark, Kafka, Hive, or similar), orchestration tools (dagster, airflow, oozie, luigi), and storage(S3, Hadoop, DBFS).Experience with modern databases (PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar).Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills.Experience building CI/CD pipelines with services like Bitbucket Pipelines or GitHub Actions.Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time.An open mind to try solutions that may seem astonishing at first.An MS in Computer Science or equivalent experience.Exceptional candidates also have:Experience with Web3 tooling.Experience with artificial intelligence, machine learning, and other big data techniques.B2B software design experience.No crypto or Web3 experience? No problem! We’ll help coach you and cover any costs for educational materials for your growth.BenefitsUnlimited PTO. Competitive compensation packages. Remote friendly & flexible hours. Wellness packages for mental and physical health.


        Show more

        


        Show less",Mid-Senior level
Geopath,Data Engineer,"Applicants must be authorized to work for any employer in the United States. We are unable to sponsor, or take over sponsorship, of an employment visa at this time.About the roleReporting to the SVP, Data & Technology and Lead Data Engineer, the Data Engineer will play a critical role in supporting and advancing Geopath’s new audience measurement platform.Responsibilities:Build scalable functions, fault tolerant batch & real time data pipelines to validate/extract/transform/integrate large scale datasets inclusive of location and time series data, across multiple platformsReview current data processes to identify improvement areas: design and implement solutions to improve scalability, performance, cost efficiencies and data qualityExtend and optimize Geopath’s data platform, which spans across multiple cloud services, data warehouses, and applications in order to meet the industry’s evolving data needsResearch, evaluate, analyze and integrate new data sources and develop quick prototypes for new data productsWork closely with product owners, data architects, data scientists and application development teams to quickly define prototypes, and build new data productsDevelop a solid understanding of the nuances within Geopath’s foundational data sourcesSupport internal stakeholders including data, design, product and executive teams by assisting them with data-related technical issuesRequirements:Bachelor’s or master's degree in computer science, computer engineering, informatics, or equivalent fields3+ years of experience as a Data Engineer with demonstrated experience in data modeling, ETL, data warehouse/data lakes, and consolidating data from multiple data sources3+ years of experience with SQL, solid experience in writing stored procedures and UDFs, familiarity with Snowflake’s data warehouse a plus2+ years of experience in building and optimizing scalable and robust big data pipelines in Apache Airflow or other workflow engines and fluent in Python or Java programmingGood working knowledge in data partition and query optimizationExperience or desire to work in a start-up environment, good team player, and can work independently with minimal supervisionGreat analytical and problem-solving skills, eager to learn new technologies and willing to wear different hats in a small team settingGreat communication and organizational skillsExpected salary for this role is between $75,000-$140,000 per year, depending on experience.About Geopath Inc.Established in 1933, Geopath, (previously the Traffic Audit Bureau for Media Measurement Inc.,) has been the gold standard in providing media auditing and audience measurement services for the Out of Home industry in the US for 90 years. Geopath has now expanded its historical mission and is looking to the future by modernizing its mission critical, industry-standard media audience rating platform while embracing the digital era. In addition to being the industry’s media auditing solution, Geopath also analyzes people’s movement data, develops deep consumer insights, innovations and advanced media research methodologies to uncover critical insights about how consumers engage with Out of Home advertising across a multi-channel ecosystem and in the physical world.


        Show more

        


        Show less",Mid-Senior level
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insurance.Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
,,,
UCLA Health,Junior Software Engineer -  Jonsson Cancer Center,"DescriptionThe newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Center is seeking a programmer/analyst with research and development experience. This position will be working with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer. Our team is passionate about applying their knowledge of software-development and design to improve scientific research. We develop scalable and distributed software solutions that maximize utilization of both local high-performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. This requires a strong background in computer science, providing a platform for technical leadership, but linked to strong personal communication and leadership skills, to help ensure insights are broadly adopted. The successful candidate will be helping us perform research that will transform the lives of cancer patients. Your responsibilities will be to use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be rewsponsible for one major and several minor projects at any point in time. We are in a rapid growth-phase, and the successful candidate will be involved in hiring of new team members. Beyond your strong inter-personal skills and computer science background, you will have experience with either systems software, databases or algorithms, linked to implementation skills at least one of C++, R, Perl or Python. You will be comfortable in UNIX/Linux environments and using continuous integration and CASE tools. Salary Range: $5525-$10925 Monthly
      

        Show more

        


        Show less",Entry level
Diverse Lynx,Jr Data Engineer,"Job DescriptionSkills (Must Have):Python – Ingestion/manipulation of large datasets to S3 using pandasPython – Consumption of data from REST APIs using requestsAny language (Most preferably Python) – Small automation tasks within AWS S3, Glue, AthenaExperience developing in AWS. Key services: S3, Lambda, Athena, Step Functions, GlueGeneral familiarity with a variety of other systems such as Oracle/Postgres, REST APIs, SplunkDeployment of resources to AWS using ServerlessGeneral understand of Infrastructure as CodeSkills (Nice To Have)AI/Client experience in SagemakerGeneral knowledge of cyber security practices and frameworksExperience writing complex queries in Presto/HadoopDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.


        Show more

        


        Show less",Mid-Senior level
,,,
Ryan,Data Engineer I,"The Data Engineer (DE) position requires a creative problem solver who is passionate about client service. This role will work with the Project Manager and Senior Data Engineers to produce timely, best-in-class deliverables. This role specializes in obtaining, reconciling, analyzing, and preparing data for use in performing sales tax consulting engagements with emphasis on preparing audit ready data populations for the Service Delivery teams quickly and efficiently.PeopleDuties and Responsibilities:Creates a positive team member experience.Demonstrates strong written and verbal communication skills, displays a positive demeanor and team spirit.Plays a key role in driving collaboration across team members, other practices and outside entities.ClientAssists senior team members in retrieving data from client systems.Assists clients and Ryan consultants in data analysis and manipulation.Travels to client sites to assist client in gathering additional data and other necessary documentation.Collaborates with remote and local teammates to ensure efficient design and timely completion of deliverables.Assist senior team members in preparing client and project correspondence.ValueAssists in the acquisition, extraction, and transfer of client data.Analyzes data from client accounting systems to verify accuracy and completeness.Develops and deploys data extraction technologies to perform data extractions from client systems.Manipulates data using Microsoft® Access, SQL, and proprietary software.Assists with the installation of data extraction tools such as Ryan eExtract®, for various ERP systems.Analyzes large data at terabyte scale using modern database technologies.Develops code in Java or Python to support ETL process and contribute to existing code bases. Performs other duties as assigned.Education And ExperienceBS or MBA preferred in information systems or computer science. Other STEM degrees with relevant work experience or coursework are considered.1-3 years of full-time work experience is a plus. Client facing or consulting experience is considered a differentiator.Experience with an enterprise or NoSQL database is a plus.Implementation experience with a major ERP system is a plus.Computer SkillsThe candidate must have a strong command of SQL and uses ETL software such as Visual Studio to build and execute SSIS packages for data manipulation, loading, and processing as well as either Java or Python to perform successfully. Ability to work in the Microsoft Office suite is required.Certificates And LicensesValid driver’s license required.Supervisory ResponsibilitiesThe position requires no direct supervisory responsibilities.Work EnvironmentStandard indoor working environment.Occasional long periods of sitting while working at computer.Position requires regular interaction with employees and clients both in person and via e-mail and telephone.Independent travel requirement: 30 to 40%. Compensation For certain California based roles, the base salary hiring range for this position is $72,069 - $88,044For other California based locations, the base salary hiring range for this position is $66,033 - $80,707For Colorado based roles, the base salary hiring range for this position is $63,032 - $77,039For New York based roles, the base salary hiring range for this position is $72,036 - $88,044For Washington based roles, the base salary hiring range for this position is $66,033 - $80,707The Company makes offers based on many factors, including qualifications and experience.Equal Opportunity Employer: disability/veteran


        Show more

        


        Show less",Entry level
,,,
,,,
2Bridge Partners,Data Engineer,"Well known NY based Non-Profit seeks a Data Engineer.An ideal candidate will have strong data engineering skills with both SQL and Python.This position can be based out of NYC or be 100% remote.Compensation includes salary, excellent medical, dental, vision benefits, pto, 401k, a quality of life oriented work life balance, and lots of other benefits.ResponsibilitiesThis position will work within the enterprise data engineering team to continuously build, maintain, and improve the design, performance, reliability, scalability, security, and architecture of enterprise data products.Responsibilities include:Develop robust database solutions, data integration (ETL, ELT) packages, automation, performance monitoring, SQL coding, database tuning and optimizations, security management, capacity planning, database HA, and database DR solutions across required data platforms.Conduct in-depth data analysis to support data product design.Identify and resolve production database and data integration issues.Collaborates closely with data quality, application, and visualization teams to deliver high-quality data products.Participate in code review, QA, release, and continuous deployment processes.Qualifications:Bachelors Degree in Computer Science or other quantitative disciplines such as Science, Statistics, Economics or Mathematics.5+ years of progressive database development experience with the Microsoft SQL Server family suite.Experience with data modeling and data architecture principles for both analytics and transactional systems.Hands-on experience with SQL Server, Oracle, or other RDBMS required.Python ScriptingExperience with Cloud Data PlatformsPreferred Experience:Scripting experience in Powershell, C#, Java, and/or R.Experience in the Microsoft Azure technology stack is a plus.Experience with data analytics and visualization is a plus.Preferred Knowledge:Intermediate knowledge of OLTP and OLAP database designIntermediate knowledge of data modeling (normalized and dimensional)Intermediate knowledge of ETL, ELT architecture especially for real and near-real-time scenariosIntermediate knowledge of Data Architecture implementation patternsAnticipated salary for candidates living in the NY Metro market in the 140,000 to 160,000 range.


        Show more

        


        Show less",Mid-Senior level
,,,
,,,
,,,
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Geopath,Data Engineer,"Applicants must be authorized to work for any employer in the United States. We are unable to sponsor, or take over sponsorship, of an employment visa at this time.About the roleReporting to the SVP, Data & Technology and Lead Data Engineer, the Data Engineer will play a critical role in supporting and advancing Geopath’s new audience measurement platform.Responsibilities:Build scalable functions, fault tolerant batch & real time data pipelines to validate/extract/transform/integrate large scale datasets inclusive of location and time series data, across multiple platformsReview current data processes to identify improvement areas: design and implement solutions to improve scalability, performance, cost efficiencies and data qualityExtend and optimize Geopath’s data platform, which spans across multiple cloud services, data warehouses, and applications in order to meet the industry’s evolving data needsResearch, evaluate, analyze and integrate new data sources and develop quick prototypes for new data productsWork closely with product owners, data architects, data scientists and application development teams to quickly define prototypes, and build new data productsDevelop a solid understanding of the nuances within Geopath’s foundational data sourcesSupport internal stakeholders including data, design, product and executive teams by assisting them with data-related technical issuesRequirements:Bachelor’s or master's degree in computer science, computer engineering, informatics, or equivalent fields3+ years of experience as a Data Engineer with demonstrated experience in data modeling, ETL, data warehouse/data lakes, and consolidating data from multiple data sources3+ years of experience with SQL, solid experience in writing stored procedures and UDFs, familiarity with Snowflake’s data warehouse a plus2+ years of experience in building and optimizing scalable and robust big data pipelines in Apache Airflow or other workflow engines and fluent in Python or Java programmingGood working knowledge in data partition and query optimizationExperience or desire to work in a start-up environment, good team player, and can work independently with minimal supervisionGreat analytical and problem-solving skills, eager to learn new technologies and willing to wear different hats in a small team settingGreat communication and organizational skillsExpected salary for this role is between $75,000-$140,000 per year, depending on experience.About Geopath Inc.Established in 1933, Geopath, (previously the Traffic Audit Bureau for Media Measurement Inc.,) has been the gold standard in providing media auditing and audience measurement services for the Out of Home industry in the US for 90 years. Geopath has now expanded its historical mission and is looking to the future by modernizing its mission critical, industry-standard media audience rating platform while embracing the digital era. In addition to being the industry’s media auditing solution, Geopath also analyzes people’s movement data, develops deep consumer insights, innovations and advanced media research methodologies to uncover critical insights about how consumers engage with Out of Home advertising across a multi-channel ecosystem and in the physical world.


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
CVS Health,Data Engineer,"Job DescriptionAssists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsApplies understanding of key business drivers to accomplish own workUses expertise, judgment and precedents to contribute to the resolution of moderately complex problemsLeads portions of initiatives of limited scope, with guidance and directionWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with client team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support clients and other internal customersIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsPay RangeThe typical pay range for this role is:Minimum: $ 70,000Maximum: $ 140,000Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.Required Qualifications1+ years of progressively complex related experienceExperience with bash shell scripts, UNIX utilities & UNIX CommandsPreferred QualificationsAbility to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsStrong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentExperience building data transformation and processing solutionsHas strong knowledge of large-scale search applications and building high volume data pipelinesEducationBachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related disciplineMaster’s degree or PhD preferredBusiness OverviewBring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
      

        Show more

        


        Show less",Entry level
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
,,,
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
UCLA Health,Junior Software Engineer -  Jonsson Cancer Center,"DescriptionThe newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Center is seeking a programmer/analyst with research and development experience. This position will be working with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer. Our team is passionate about applying their knowledge of software-development and design to improve scientific research. We develop scalable and distributed software solutions that maximize utilization of both local high-performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. This requires a strong background in computer science, providing a platform for technical leadership, but linked to strong personal communication and leadership skills, to help ensure insights are broadly adopted. The successful candidate will be helping us perform research that will transform the lives of cancer patients. Your responsibilities will be to use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be rewsponsible for one major and several minor projects at any point in time. We are in a rapid growth-phase, and the successful candidate will be involved in hiring of new team members. Beyond your strong inter-personal skills and computer science background, you will have experience with either systems software, databases or algorithms, linked to implementation skills at least one of C++, R, Perl or Python. You will be comfortable in UNIX/Linux environments and using continuous integration and CASE tools. Salary Range: $5525-$10925 Monthly
      

        Show more

        


        Show less",Entry level
Diverse Lynx,Jr Data Engineer,"Job DescriptionSkills (Must Have):Python – Ingestion/manipulation of large datasets to S3 using pandasPython – Consumption of data from REST APIs using requestsAny language (Most preferably Python) – Small automation tasks within AWS S3, Glue, AthenaExperience developing in AWS. Key services: S3, Lambda, Athena, Step Functions, GlueGeneral familiarity with a variety of other systems such as Oracle/Postgres, REST APIs, SplunkDeployment of resources to AWS using ServerlessGeneral understand of Infrastructure as CodeSkills (Nice To Have)AI/Client experience in SagemakerGeneral knowledge of cyber security practices and frameworksExperience writing complex queries in Presto/HadoopDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.


        Show more

        


        Show less",Mid-Senior level
Sayari | Commercial Risk Intelligence,Data Engineer,"Sayari is looking for Data Engineers to join our growing team! We are hiring at all levels and encourage junior through senior level candidates to apply. As a member of Sayari's data team you will work with our Product and Software Engineering teams to collect data from around the globe, maintain existing ETL pipelines, and develop new pipelines that power Sayari Graph.About Sayari:Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal & advisory service providers, multinationals, journalists, and governments. We are building world-class SaaS products that help our clients glean insights from vast datasets that we collect, extract, enrich, match and analyze using a highly scalable data pipeline. From financial intelligence to anti-counterfeiting, and from free trade zones to war zones, Sayari powers cross-border and cross-lingual insight into customers, counterparties, and competitors. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering.Our company culture is defined by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, encourage training and learning opportunities, and reward initiative and innovation. If you enjoy working with supportive, high-performing, and curious teams, Sayari is the place for you.Position DescriptionSayari’s flagship product, Sayari Graph, provides instant access to structured business information from hundreds of millions of corporate, legal, and trade records. As a member of Sayari's data team you will work with our Product and Software Engineering teams to collect data from around the globe, maintain existing ETL pipelines, and develop new pipelines that power Sayari Graph.RequirementsWhat You Will Need:Professional experience with Python and a JVM language (e.g., Scala)2+ years of experience designing and maintaining ETL pipelinesExperience using Apache Spark and Apache AirflowExperience with SQL and NoSQL databases (e.g., columns stores, graph, etc.)Experience working on a cloud platform like GCP, AWS, or AzureExperience working collaboratively with gitWhat We Would Like:Understanding of Docker/KubernetesUnderstanding of or interest in knowledge graphsWho You Are:Experienced in supporting and working with cross-functional teams in a dynamic environmentInterested in learning from and mentoring team members Passionate about open source development and innovative technologyBenefitsA collaborative and positive culture - your team will be as smart and driven as youLimitless growth and learning opportunities A strong commitment to diversity, equity, and inclusion Performance and incentive bonuses Outstanding competitive compensation and comprehensive family-friendly benefits, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and parental leave.Conference & Continuing Education Coverage Team building events & opportunitiesSayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply.


        Show more

        


        Show less",Entry level
Ryan,Data Engineer I,"The Data Engineer (DE) position requires a creative problem solver who is passionate about client service. This role will work with the Project Manager and Senior Data Engineers to produce timely, best-in-class deliverables. This role specializes in obtaining, reconciling, analyzing, and preparing data for use in performing sales tax consulting engagements with emphasis on preparing audit ready data populations for the Service Delivery teams quickly and efficiently.PeopleDuties and Responsibilities:Creates a positive team member experience.Demonstrates strong written and verbal communication skills, displays a positive demeanor and team spirit.Plays a key role in driving collaboration across team members, other practices and outside entities.ClientAssists senior team members in retrieving data from client systems.Assists clients and Ryan consultants in data analysis and manipulation.Travels to client sites to assist client in gathering additional data and other necessary documentation.Collaborates with remote and local teammates to ensure efficient design and timely completion of deliverables.Assist senior team members in preparing client and project correspondence.ValueAssists in the acquisition, extraction, and transfer of client data.Analyzes data from client accounting systems to verify accuracy and completeness.Develops and deploys data extraction technologies to perform data extractions from client systems.Manipulates data using Microsoft® Access, SQL, and proprietary software.Assists with the installation of data extraction tools such as Ryan eExtract®, for various ERP systems.Analyzes large data at terabyte scale using modern database technologies.Develops code in Java or Python to support ETL process and contribute to existing code bases. Performs other duties as assigned.Education And ExperienceBS or MBA preferred in information systems or computer science. Other STEM degrees with relevant work experience or coursework are considered.1-3 years of full-time work experience is a plus. Client facing or consulting experience is considered a differentiator.Experience with an enterprise or NoSQL database is a plus.Implementation experience with a major ERP system is a plus.Computer SkillsThe candidate must have a strong command of SQL and uses ETL software such as Visual Studio to build and execute SSIS packages for data manipulation, loading, and processing as well as either Java or Python to perform successfully. Ability to work in the Microsoft Office suite is required.Certificates And LicensesValid driver’s license required.Supervisory ResponsibilitiesThe position requires no direct supervisory responsibilities.Work EnvironmentStandard indoor working environment.Occasional long periods of sitting while working at computer.Position requires regular interaction with employees and clients both in person and via e-mail and telephone.Independent travel requirement: 30 to 40%. Compensation For certain California based roles, the base salary hiring range for this position is $72,069 - $88,044For other California based locations, the base salary hiring range for this position is $66,033 - $80,707For Colorado based roles, the base salary hiring range for this position is $63,032 - $77,039For New York based roles, the base salary hiring range for this position is $72,036 - $88,044For Washington based roles, the base salary hiring range for this position is $66,033 - $80,707The Company makes offers based on many factors, including qualifications and experience.Equal Opportunity Employer: disability/veteran


        Show more

        


        Show less",Entry level
Oshi Health,Data Engineer,"Do you love to work with data, finding ways to make it reportable, and building models that will add clinical and commercial value for the future?Do you want to bring your skills and experience to a growth stage engineering team, and help set us up for smart expansion?Are you excited by the prospect of having a high-visibility high-impact role in a fast-moving startup?Are you passionate about healthcare, and looking to create a revolutionary new approach to digestive healthcare with a radically better patient experience?If so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company.Oshi Health is revolutionizing GI care with a virtual clinic that provides easy, convenient access to a multidisciplinary care team including a GI Physician, Registered Dietician, Mental Health Professional, and Health Coach that takes a whole-person approach to diagnosing, managing and treating digestive health conditions. Our care is built on the latest evidence-based protocols and is delivered virtually through an app, secure messaging and telehealth visits with the care team. NOTE: Oshi is a fully remote company, with team members all over the US.What You’ll Do:This role will be a perfect fit if you enjoy learning the entire stack and taking on interdisciplinary challenges. A primary focus will be building out a data engineering program, including ETL, data governance, sanitization, and data operations. This part of your responsibilities will be a balance of executing data operations, and building automation to make those operations scalable.You will also have the opportunity to join engineers on the frontend end (React Native and React.js), backend (Node.js Lambdas) and Salesforce to help build the Oshi platform. Experience with any of these technologies is a plus, but more important is an enthusiasm to adapt and learn the ones that are new to you.What you’ll do: build the Oshi data programImplement and maintain data pipelines using Stitch, Databricks, and other scripting as needed to feed a PostgreSQL schema supporting Tableau reportingSupport users of Tableau by updating data sources or modifying inbound inputs as needed deliver critical BI reportsOwn the Oshi data model, ensuring that new features built and new technologies adopted serve the needs of the clinical, commercial, product, and engineering teamsManage ETL of client eligibility files and other data, to make them available for Oshi use in a secure and timely manner. Wherever possible, replace bespoke processes with automationOnce you have a understanding of Oshi’s requirements, design and implement a data strategy, (with your recommendation of approach and products,) to meet the needs of Oshi’s analytics, commercial, and clinical business linesYour work will also include:AWS maintenance and administration Writing technical documentation to outline designs for forthcoming features, outlining the implementation across all technology layersMeeting with colleagues in Strategy, Product, and Clinical to support their needs from the Engineering group.Production support responsibilities (shared with the entire engineering team) responding to alerts in Datadog, reviewing and troubleshooting issuesOur tech stack:Mobile Platforms Supported: iOS & AndroidCross-Platform Mobile Language: React NativeOther Languages: React-js, HTML, CSS, Java (Salesforce Apex), Node.js (Lambda)Systems: Salesforce, AWS Amplify / Cognito / LambdaYour Profile:A minimum of 3+ years of professional experienceBachelor's Degree or equivalent experienceGood interpersonal and relationship skills that include a positive attitudeSelf-starter who can find a way forward even when the path is unclear.Team player AND a leader simultaneously.What You’ll Bring to the Team:Passionate about creating value that changes people's livesMake low-level decisions quickly while being patient and methodical with high-level onesAre curious and passionate about digging into new technologies with a knack for picking them up quicklyAdept at prioritizing value and shipping complex products while coordinating across multiple teamsLove working with a diverse set of engineers, product managers, designers, and business partnersStrive to excel, innovate and take pride in your workWork well with other leadersAre a positive culture driverExcited about working in a fast-paced, startup cultureExperience in a regulated industry (healthcare, finance, etc.) a plusand perks:We’re revolutionizing GI care — and our employees are driving the change. We’re a hard-working and fun-loving team, committed to always learning and improving, and dedicated to doing the right thing for our members. To achieve our mission, we invest in our people:We make healthcare more equitable and accessible:Mission-driven organization focused on innovative digestive careThrive on diversity with monthly DEIB discussions, activities, and moreVirtual-first culture: Work from home anywhere in the USLive our core values: Own the outcome, Do the right thing, Be direct and open, Learn and improve, Team, Thrive on diversity We take care of our people:Competitive compensation and meaningful equityEmployer-sponsored medical, dental and vision plans Access to a “Life Concierge” through Overalls, because we know life happensTailored professional development opportunities to learn and growWe rest, recharge and re-energize:Unlimited paid time off — take what you need, when you need it13 paid company holidays to power downTeam events, such as virtual cooking classes, games, and moreRecognition of professional and personal accomplishmentsOshi Health’s Core Values:Go For ItDo the Right ThingBe Direct & OpenLearn & ImproveTEAM - Together Everyone Achieves MoreOshi Health is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Powered by JazzHRFRVWJuzRKn
      

        Show more

        


        Show less",Mid-Senior level
Geopath,Data Engineer,"Applicants must be authorized to work for any employer in the United States. We are unable to sponsor, or take over sponsorship, of an employment visa at this time.About the roleReporting to the SVP, Data & Technology and Lead Data Engineer, the Data Engineer will play a critical role in supporting and advancing Geopath’s new audience measurement platform.Responsibilities:Build scalable functions, fault tolerant batch & real time data pipelines to validate/extract/transform/integrate large scale datasets inclusive of location and time series data, across multiple platformsReview current data processes to identify improvement areas: design and implement solutions to improve scalability, performance, cost efficiencies and data qualityExtend and optimize Geopath’s data platform, which spans across multiple cloud services, data warehouses, and applications in order to meet the industry’s evolving data needsResearch, evaluate, analyze and integrate new data sources and develop quick prototypes for new data productsWork closely with product owners, data architects, data scientists and application development teams to quickly define prototypes, and build new data productsDevelop a solid understanding of the nuances within Geopath’s foundational data sourcesSupport internal stakeholders including data, design, product and executive teams by assisting them with data-related technical issuesRequirements:Bachelor’s or master's degree in computer science, computer engineering, informatics, or equivalent fields3+ years of experience as a Data Engineer with demonstrated experience in data modeling, ETL, data warehouse/data lakes, and consolidating data from multiple data sources3+ years of experience with SQL, solid experience in writing stored procedures and UDFs, familiarity with Snowflake’s data warehouse a plus2+ years of experience in building and optimizing scalable and robust big data pipelines in Apache Airflow or other workflow engines and fluent in Python or Java programmingGood working knowledge in data partition and query optimizationExperience or desire to work in a start-up environment, good team player, and can work independently with minimal supervisionGreat analytical and problem-solving skills, eager to learn new technologies and willing to wear different hats in a small team settingGreat communication and organizational skillsExpected salary for this role is between $75,000-$140,000 per year, depending on experience.About Geopath Inc.Established in 1933, Geopath, (previously the Traffic Audit Bureau for Media Measurement Inc.,) has been the gold standard in providing media auditing and audience measurement services for the Out of Home industry in the US for 90 years. Geopath has now expanded its historical mission and is looking to the future by modernizing its mission critical, industry-standard media audience rating platform while embracing the digital era. In addition to being the industry’s media auditing solution, Geopath also analyzes people’s movement data, develops deep consumer insights, innovations and advanced media research methodologies to uncover critical insights about how consumers engage with Out of Home advertising across a multi-channel ecosystem and in the physical world.


        Show more

        


        Show less",Mid-Senior level
"Verticalmove, Inc",Data Engineer,"We are currently hiring a Senior Data Platform Engineer to help grow our company and ensure our mission is achieved!This role is a work from home position and can be performed remotely anywhere in the continental US or in one of our corporate locations in Utah or Arizona.WE ARE: Our Data Platforms embodies the modernity and transformational vision that is core to our business evolution. As passionate and hungry technical experts, we progress through technology. We take pride in our engineering, daily progress, and bringing others along as we improve. We experiment, fail fast, and drive to delivery.YOU ARE: A self-starter mindset to proactively take ownership and execute work without daily oversight and excited to develop your data administration & data DevOps skills. We have a great team of engineers building next-generation data platforms. You are motivated by challenges and thrive with continuous innovation.YOUR DAY-TO-DAY:Coach and develop fellow team membersWorking in an agile-scrum development environmentWork with engineers and leaders to promote a shared vision of our data platform & architectureTrack operating metrics for our data platforms, driving improvements and making trade-offs among competing constraints. We use MS SQL Server, AWS RDS (Postgres, MariaDB) MongoDB, DynamoDB, Redis, Rabbit MQ, AWS managed messaging/eventing systems (Kafka, Kinesis, SQS, SNS) to name a few of our platformsBe a strategic player in designing enterprise-wide data infrastructureBuilds robust systems with an eye on the long-term maintenance and support of the applicationDiscover automation opportunities to replace manual processes using PowerShell, Terraform, Python etcBuild out frameworks for data administration /data deployment /monitoring where neededLeverage reusable code modules to solve problems across the team and organizationCreate and maintain automated pipelines to deploy changes via Octopus, CircleCI, Azure DevOps and JenkinsProactive and reactive performance analysis, monitoring, troubleshooting and resolution of escalated issuesParticipate in the team on-call rotationsYOU’LL BRING:Used multiple data platforms and can define which is optimal for a given scenario (such as document databases, RDBMS, caching, eventing, etc. On-prem or cloud)An engineering mindset when developing a solution using PowerShell / Python or SQL. In depth knowledge and use of tools such as dbatools and other modules is a plusUnderstand how to fully automate systems using infrastructure/configuration as code technologies (we use Terraform, CloudFormation, Ansible, SaltStack)Experience working with CI/CD pipeline and tools preferably Octopus, CircleCI, AzureDevOps and JenkinsProven ability to mentor and coach engineersThrive on a high level of autonomy and responsibility, while having the ability to dig into technical details to inform decisionsAdvanced Sql Server Knowledge and experience in performance analysis and tuning skills such as tracing and profiling and Dynamic Management Views and Functions (DMVs) and other third-party tools like SentryOne to ensure high levels of performance, availability, and securityKnowledge of database deployment using DACPAC via pipelinesExperience with enterprise features of SQL Server: AlwaysOn Availability Groups, clustering, Disaster RecoveryContribute to the management and reliability of database HA/DR processesYOU MIGHT ALSO HAVE:Ability to clearly articulate complex subjects to leadership and others not familiar with this spaceExperience with code-based data developmentA self-starter mindset to proactively take ownership and execute work without daily oversightExperience in AWS cloud-based solutionsWE OFFER: Competitive Compensation; Eligible for STI + LTIFull Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursEmployee Resource Groups (Black Inclusion Group, Women in Leadership, PRIDE, Adelante)Employee Stock Purchase ProgramTuition ReimbursementCharitable Gift MatchingJob required equipment and services


        Show more

        


        Show less",Mid-Senior level
Zortech Solutions,Data Engineer-US,"Role: Data Engineer (ETL, Python)Location: Dallas, TX (Hybrid)Duration: 6+ MonthsResponsibilitiesJob DescriptionStrong Experience With ETL, Python And Data EngineerHybrid: 2 days office, 3 days remote and need only local candidatesWork with business stakeholders, Business Systems Analysts and Developers to ensure quality delivery of software.Interact with key business functions to confirm data quality policies and governed attributes.Follow quality management best practices and processes to bring consistency and completeness to integration service testingDesigning and managing the testing AWS environments of data workflows during development and deployment of data productsProvide assistance to the team in Test Estimation & Test PlanningDesign, development of Reports and dashboards.Analyzing and evaluating data sources, data volume, and business rules.Proficiency with SQL, familiarity with Python, Scala, Athena, EMR, Redshift and AWS.No SQL data and unstructured data experience.Extensive experience in programming tools like Map Reduce to HIVEQLExperience in data science platforms like Sage Maker/Machine Learning Studio/ H2O.Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.Interpret and analyses data from various source systems to support data integration and data reporting needs.Experience in testing Database Application to validate source to destination data movement and transformation.Work with team leads to prioritize business and information needs.Develop complex SQL scripts (Primarily Advanced SQL) for Cloud ETL and On prem.Develop and summarize Data Quality analysis and dashboards.Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.Execute testing of data analytic and data integration on time and within budget.Work with team leads to prioritize business and information needsTroubleshoot & determine best resolution for data issues and anomaliesExperience in Functional Testing, Regression Testing, System Testing, Integration Testing & End to End testing.Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsRequirementsExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience testing Cloud/On Prem ETL (e.g. Abinito, Informatica, SSIS, DataStage, Alteryx, Glu)Extensive experience using Python scripting and AWS and Cloud Technologies.Extensive experience using Athena, EMR , Redshift and AWS and Cloud TechnologiesAPI/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API.Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.AWS/Cloud - Jenkins/ Gitlab/ EC2 machine, S3 and building Jenkins and CI/CD pipelines, SauceLabs.API/Rest API - Rest API and Micro Services using JSON, SoapUIExtensive experience in DevOps/Data Ops space.Strong experience in working with DevOps and build pipelines.Strong experience of AWS data services including Redshift, Glue, Kinesis, Kafka (MSK) and EMR/ Spark, Sage Maker etcExperience with technologies like Kubeflow, EKS, DockerExtensive experience using No SQL data and unstructured data experience like MongoDB, Cassandra, Redis, Zookeeper.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience using Jenkins and GitlabExperience using both Waterfall and Agile methodologies.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)Good To HaveGood to have Java knowledgeKnowledge of integrating with Test Management ToolsKnowledge in Salesforce
      

        Show more

        


        Show less",Mid-Senior level
Nexus Staff Inc.,Data Engineer,"Our team relies on clean datasets accessible via the latest tools to answer questions that are often not simple or clear. You will be creating solutions that will help derive value from our rich datasets that will solve tough problems impacting consumers across geographies & industries (healthcare, retail/ consumer, telecom, industrial) driving technology transformation. At the core of our team, we believe data is best used to create transparency & generate communal value.  What You’ll Do: · Design, Build & ImplementDesign & build batch, event driven and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWSImplement large-scale data platforms to meet the analytical & operational needs across various organizationsBuild products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutionsBuild streaming ingestion processes to efficiently read, process, analyze & publish data for real-time need of applications and data science modelsPerform analyses of large structured and unstructured data to solve multiple & complex business problemsInvestigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise Understand business use cases to design engineering routines to affect the outcomesReview & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms.Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines   Who You Are : An Engineer interested in working in batch, event driven and streaming processing environments A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure & AWSA data parser expert who gets excited about structuring and re-structuring datasets programmatically A doer who loves to produce meaningful analytic insights for an innovative, data-intensive productsAlways curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologiesTechnologist who loves studying software platforms with an eye towards modernizing the architectureBeliever in transparency, communication, and teamworkLove to learn and not afraid to take ownershipNexus Staffing is committed to diversity, inclusion, and equal opportunity. We take affirmative steps to ensure that all programs and services are open to all Americans, free of discrimination based on protected class status, including race, religion/creed, color, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin (including limited English proficiency), age, political affiliation or belief, military or veteran status, disability, predisposing genetic characteristics, marital or family status, domestic violence victim status, arrest record or criminal conviction history, or any other impermissible basis.


        Show more

        


        Show less",Mid-Senior level
Patterned Learning AI,Data Engineer (Entry Level ),"REMOTE (US/Canada Residing people only, with work permit)Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!About The JobRequired Skills and Experience:Experience in writing cloud-based softwareExpertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.Experience delivering software products built using PythonExperience using professional software development practices, including: Agile processes, CI/CD, etc)Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)Special Benefits You Will LoveFlexible vacation, paid holidays, and paid sick days401(k) with up to 2% employer match (no match)Health, vision, and dental insurance.Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
      

        Show more

        


        Show less",Entry level
AMERICAN EAGLE OUTFITTERS INC.,Data Engineer Intern – Summer 2023,"Job DescriptionPOSITION TITLE: Data Engineer InternPosition SummaryThe Data Engineer Intern will help to enable data as the forefront of all business process and decisions. You will be part of a team of strong technologists passionate about our roadmap to deliver highly available and scalable data pipelines and solutions in the Google Cloud Platform (GCP) allowing teams across Digital, Stores, Marketing, Supply Chain, Finance, and Human Resources to make real-time decisions based on consistent, complete, and correct data using Data Quality rules. Your contributions will include supporting improvements in our data platform, building frameworks for security, automation and optimization, and curating data to be consumed by Data Science, Data Insights, and Advanced Analytics teams across the organization using a suite of sophisticated cloud tools and open-source technologies.Responsibilities Guided work using Google Cloud Platform (GCP) environment to perform the following:Develop highly available, scalable data pipelines and applications to support business decisions Pipeline development and orchestration using Cloud Data Fusion/CDAP, Cloud Composer/Airflow/Python, DataflowBig Query table creation and query optimizationCloud Function’s for event-based triggeringCloud Monitoring and AlertingPub/Sub for real-time messagingCloud Data Catalog build-outWork in an agile environment applying SDLC principles and SCRUM methodologies utilizing tools such as Jira, Wiki, Bitbucket/GitHub and BambooGuided work around software and product security, scalability, general data warehousing principles, documentation practices, refactoring and testing techniquesUse Terraform for infrastructure automation and provisioning.QualificationsBachelor/Master’s degree in MIS, Computer Science, or a related discipline Experience / training using Java or PythonUnderstanding of Big Data ETL Pipelines and familiar with Dataproc, Dataflow, Spark, or HadoopProficient ANSI SQL skills Pay/Benefits InformationActual starting pay is determined by various factors, including but not limited to relevant experience and location.Subject to eligibility requirements, associates may receive health care benefits (including medical, vision, and dental); wellness benefits; 401(k) retirement benefits; life and disability insurance; employee stock purchase program; paid time off; paid sick leave; and parental leave and benefitsPaid Time Off, paid sick leave, and holiday pay vary by job level and type, job location, employment classification (part-time or full-time / exempt or non-exempt), and years of service. For additional information, please click here .AEO may also provide discretionary bonuses and other incentives at its discretion.About UsAmerican Eagle - We're an American jeans and apparel brand that's true in everything we do. Rooted in authenticity, powered by positivity, and inspired by our community – we welcome all and believe that putting on a really great pair of #AEjeans gives you the freedom to be true to you. Because when you're at your best, you put good vibes out there, and get good things back in return. AE. True to you.American Eagle Outfitters® (NYSE: AEO) is a leading global specialty retailer offering high-quality, on-trend clothing, accessories and personal care products at affordable prices under its American Eagle® and Aerie® brands.The company operates stores in the United States, Canada, Mexico, and Hong Kong, and ships to 81 countries worldwide through its websites. American Eagle and Aerie merchandise also is available at more than 200 international locations operated by licensees in 24 countries.AEO is an Equal Opportunity Employer and is committed to complying with all federal, state and local equal employment opportunity (""EEO"") laws. AEO prohibits discrimination against associates and applicants for employment because of the individual's race or color, religion or creed, alienage or citizenship status, sex (including pregnancy), national origin, age, sexual orientation, disability, gender identity or expression, marital or partnership status, domestic violence or stalking victim status, genetic information or predisposing genetic characteristics, military or veteran status, or any other characteristic protected by law. This applies to all AEO activities, including, but not limited to, recruitment, hiring, compensation, assignment, training, promotion, performance evaluation, discipline and discharge. AEO also provides reasonable accommodation of religion and disability in accordance with applicable law.


        Show more

        


        Show less",Internship
Zortech Solutions,Data Engineer with SQL-US/Canada,"Role: Data Engineer with SQLLocation: Bellevue, WA/Remote (PST zone)/CanadaDuration: 6+ MonthsJob Description Data engineer + SQL masteryData normalization and denormalization principles and practiceAggregates, Common Table Expressions, Window FunctionsMulti-column joins, self joins, preventing row explosionsExperience with Postgres is a plusNeeds to understand database connectivityHow to connect to a databaseHow to explore a databaseHow to perform multiple operation in a single transactionNeeds to know how to do the basics with gitEnough familiarity with Azure cloud computing concepts to get things doneExperience with Databricks and/or Delta Lake a plusExperience with Azure Data Factory a plusSome level of scripting (preferably in python) is beneficialSome level of geospatial knowledge a plusSome level of geospatial SQL knowledge an extra special plus


        Show more

        


        Show less",Mid-Senior level
"Digible, Inc",Junior Data Engineer,"Company OverviewPrivately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.The RoleDigible, Inc. is looking for an Junior Software Engineer to join our team!We are seeking a highly motivated and detail-oriented Junior Data Engineer to join our growing team. As a Junior Data Engineer, you will be responsible for assisting with the development, implementation, and maintenance of our data infrastructure. You will work closely with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources.Responsibilities:Assist with the design, development, and maintenance of our data infrastructure using Python, Prefect, Snowflake, Google Cloud, and AWS. Collaborate with our Data Engineering team to ensure data quality, accuracy, and completeness across all of our data sources. Develop and maintain ETL pipelines to extract, transform, and load data from various sources into our data warehouse. Assist with the implementation of data security and governance policies. Monitor and troubleshoot data issues as they arise. Continuously seek ways to improve our data infrastructure and processes. Requirements:Bachelor's degree in Computer Science, Data Science, or a related field. Experience with Python, SQL, and data modeling. Familiarity with data warehousing concepts and ETL processes. Experience working with cloud-based platforms such as Google Cloud and AWS. Strong problem-solving skills and attention to detail. Excellent communication and teamwork skills. Expectations:Ability to learn and adapt quickly to new technologies and processes. Work collaboratively with our Data Engineering team to meet project goals and deadlines. Demonstrate a high level of professionalism and dedication to quality work. Communicate effectively with cross-functional teams to ensure project success. Success Metrics:Deliver high-quality data infrastructure projects on time and within scope. Ensure data accuracy and completeness across all data sources. Maintain a high level of data security and governance. Continuously seek ways to improve our data infrastructure and processes. Collaborate effectively with cross-functional teams to meet project goals and objectives. Core ValuesAuthenticity - The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.Curiosity - The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.Focus - The collective will to remain completely devoted and ultimately accountable to our deliverables.Humility - The recognition and daily practice that ""we"" is always greater than ""I"".Happiness - The decision to prioritize passion and love for what we do above everything else.Perks and such:4-Day Work Week (32 Hour Work Week)WFA (Work From Anywhere)Profit Sharing BonusWe offer 3 weeks of PTO as well as Sick leave, and Bereavement. We offer 10 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Thanksgiving + day after, Christmas Eve, and Christmas)401(k) + Match50% employer paid health benefits, including Medical, Dental, and Vision. We provide $75/ month reimbursement for Physical WellnessWe provide $75/ month reimbursement for Mental Wellness$1000/year travel fund for employees who have been with Digible 3+ yearsMonthly subscription for financial wellnessDog-Friendly OfficePaid Parental LeaveCompany Sponsored Social EventsCompany Provided Lunches, SnacksEmployee Development Program


        Show more

        


        Show less",Mid-Senior level
,,,
StaffSource,ETL and Data Engineer,"The Data Engineer develops data acquisition, storage, processing, and integration solutions for operational, reporting, and analytical purposes. This person is often called upon to generate production data solutions to support new rate analyses, operational reports, integrations with external entities, and new functionality in transactional systems. The Data Engineer will employ sound data management fundamentals and best practices to transform raw data resources into enterprise assets. Duties and responsibilities include the following:Design data structures and solutions to support transactional data processing, batch and real-time data movement, and data warehousingPerform ad-hoc query and analysis on structured and unstructured data from a variety of sourcesDevelop queries, data marts, and data files to support reporting and analytics efforts in IT and business unit customers. Recommend and implement data solutions to improve the usage of data assets across the organizationOptimize and operationalize prototype solutions developed by other team members or business analysts. Participate in the design and development of data services (REST, SOAP, etc.) as neededMap existing solutions developed in legacy technology to new architecture and implement modernized solutions in target-state technology stackPerform peer review and occasional QA support for solutions developed by other associates within the Data Management group or other functional areasInvestigate and document current data flow processes between corporate systems, business partners, and downstream data consumersCreate data models and technical metadata using modeling tools such as ER Studio or ErwinDocument and disseminate system interactions and data process flowsParticipate in the design and development of target-state analytics ecosystem using traditional and big data tools, as well as a mix of on-premises and cloud infrastructure Qualifications and Requirements5+ years' work experience developing high-performing data systems, adhering to established best-practicesStrong SQL development skills , creating complex queries, views, and stored proceduresSolid understanding of SQL best practicesExperience interfacing with business stakeholders to understand data and analytics needs and deliver solutionsExperience with ETL tools such as SSIS, Azure Data Factory and SQL Server (required)Experience in a variety of data technologies such as SQL Server, DB2, MongoDB (nice to have)Strong experience with relational data structures, theories, principles, and practicesExperience in Agile Scrum and / or Kanban project management frameworksExperience in creating data specifications, data catalogs, and process flow diagramsExperience developing REST or SOAP services preferred


        Show more

        


        Show less",Entry level
Travelers,"Data Engineer I (SQL, Python, Salesforce, Tealium)","R-27413Who Are We?Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$102,600.00 - $169,200.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Personal Insurance Analytics and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned.What Will Our Ideal Candidate Have?Bachelor’s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. Experience with Salesforce and Tealium Product Suite a plus.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.
      

        Show more

        


        Show less",Not Applicable
Simplex.,Data / Analytics Engineer,"This role has a preference for a local Austin, TX candidate and will be a hybrid work environment meeting in-person a few times/month. Our client is a fast-growing, Private Equity backed GovTech company that provides SaaS Software solutions to the Public Sector (City / State Local Government) and is looking for a Data Engineer to join their team. Reporting to the Director of Analytics as their first hire, you will be responsible for the ongoing development and management of the data infrastructure of the business and will be providing essential support to internal customers. This pivotal role will be in the Analytics team reporting to Finance and is aimed to drive greater efficiency in data, processes, and tools, ultimately enhancing data-driven decision-making capabilities. For someone with an interest in more of the analytics side of things, this role could also serve as a hybrid Analytics / Dashboarding role as well. This role is great for someone who perhaps hasn't yet chosen a specific specialization or path in the data realm and wants to gain exposure or opportunity in different areas - Whether BI Front End, Business Strategy, Analytics / Dashboarding, or Data Engineering there are a plethora of opportunities to learn and jump in to help the business. One of the major projects you'll be working on is how to collect data on the usage of their underlying SaaS Software products and how to marry that with their CRM data. They have different products that were built at different times so getting to and extracting that data is going to be an especially interesting challenge for this team. ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes to ingest and process data from various sourcesBuild and optimize the data warehouse in SnowflakeCollaborate with data users to understand requirements and create efficient data models and structures for seamless reporting and analysis using BI tools. Monitor and optimize data pipeline performanceImplement data governance and security best practices, manage user access, and ensure compliance is adhered toContinuously explore and evaluate new data engineering techniques and tools, including AI applications, to improve data processing and exploration capabilities. QualificationsExperience building or using data marts (sql heavy data modeling) and interacting with users that use the data. The ability to find the shortest path to making data available and widely usable. Experience building or extending a Data WarehouseStrong exposure to Data Warehouse patterns and the application of those patternsPrevious experience working with business stakeholdersStrong initiative to start new projects or take existing projects and make them better. Proficiency with Snowflake, Tableau, and Stitch, or similar data warehousing, BI, and ETL tools. Strong knowledge of SQL is required with database design and data modeling experience. Experience with Python or another programming language for data processing. Familiarity with data engineering best practices, including data quality, data governance, and data security. Interest in AI applications for data engineering and data exploration. Excellent problem-solving, communication, and collaboration skills. Extra informationCareer development opportunitiesCompetitive insurance (medical, dental, vision, and voluntary life & disability)Mental health benefits401(k) plan (company matching)Paid holidaysFlexible PTO - no accrualsPaid generous parental leaveBusiness casual environment #ZR


        Show more

        


        Show less",Entry level
Experfy,"Data Engineer, Database Engineering","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities:Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques.Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchainsSkills & QualificationsBachelor’s degree in computer science or related technical field. Masters or PhD a plus.6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this


        Show more

        


        Show less",Mid-Senior level
Sempera,Data Engineer,"Data Engineer Qualifications7+ years of experience as a Database Developer3+ years of experience as a hands on Team LeadExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectureStrong data modeling skills (relational, dimensional and flattened)5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required3-4 years of experience running ETL/ELT SSIS projects independently from end to end, requiredExperience with DevOps Repository and Git, preferredAzure Data Lake storage experience, a plusAzure Data Factory and/or Databricks experience, a plusLocation: Denver, COEmployment Type: Direct HireDuration: Full TimeWork Location: Denver / HybridPay: Based on experienceOther: PTO, Medical, Dental, Vision, Disability, Life, 401k benefits available Thank you in advance for your interest in this opportunity!If you are able to work on a W2 basis without sponsorship for ANY US employer and fit the description above, please apply. Third-Party Applications Not Accepted.


        Show more

        


        Show less",Mid-Senior level
Software Technology Inc.,Data Engineer/Data Analyst,"Hi,Hope you are doing well,We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.Role : Data Engineer/Data AnalystLocation : RemoteSkillGCP Big Query, Python, SQL and knowledge of Healthcare domain
      

        Show more

        


        Show less",Entry level
adQuadrant,Data Engineer,"adQuadrant helps DTC (direct-to-consumer) brands dream bigger. We are a trusted advisor that provides holistic, strategic omni-channel digital marketing solutions by partnering with our clients to solve the biggest challenges in terms of customer acquisition and growth. Our efforts produce tangible results backed by measurable data. We have the strategic capabilities, quantitative chops, deep creative understanding, and world-class talent with the best tools to drive revenue and profits. We are not simply a vendor checking boxes — our seasoned team serves as an extension of the companies we work with, leading strategy and execution. Our goal is to be the go-to marketing consultants for solving the biggest challenges.adQuadrant is looking for a Data Engineer to support a growing team. This role is responsible for developing, testing, and maintaining data pipelines and data architectures. You will be building and optimizing our data pipeline architecture, as well as optimizing data flows and collections for cross functional teams. The ideal candidate will enjoy optimizing data systems and building them from the ground up, you must be ready for a challenge. This role will ensure optimal data delivery architecture is consistent throughout current and ongoing projects. You must be a self-starter and enjoy supporting multiple cross-functional teams.The Ideal Candidate must have: Experience in Snowflake, architecting and building a data warehouse from scratchData pipeline (ETL tools) development and deployment experienceExtensive experience deploying and maintaining a Tableau ServerRequirementsYour Responsibilities: Consulting with the data team to get a strong understanding of the organization’s data storage needsDesigning and constructing the data warehousing system to the organization’s specificationsDefining and implementing scalable data pipelines that ingest data from various external sources, storing it in the database system, and making it useful to our data analystsImplementing processes to monitor data quality, ensuring production data is always accurate and available for data analysts and business processes that rely on itRecognize, troubleshoot, and resolve unexpected performance and process fail issues, on an ongoing basisQualifications:Bachelor’s degree in computer science, information technology, or a related field5+ years experience in data warehousing, data modeling and building data pipelinesExtensive knowledge of SQL, and coding languages, such as PythonProficiency in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDWProven work experience as an ETL developerExperience working with online marketing dataStrong project management skills and experience with Agile engineering practicesAbility to analyze a company’s big-picture data needsClear communication skillsStrategic thought and extreme attention to detailAbility to troubleshoot and solve complex technical problemsComfortable supporting distributed remote individualsBenefitsOur people come first. No jerks. No egos. Just people who like to work hard and enjoy winning as a team.Annual Compensation: $95,000 - $120,000 per yearExcellent Health Benefits (health, dental, vision, and life insurance)401K + company matchUnlimited Vacation PolicyPaid Sick Leave2 Mindfulness Days Annually2PM Fridays each week$300/ year to equip your work space with new equipment$30/ month for home internetAn extremely supportive and fun company cultureWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.


        Show more

        


        Show less",Associate
Attune ,Data Engineer,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities:Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintainsQualifications:3+ years experience in analytics, data science, or data engineering roleStrong Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with git, and Gitlab or GithubNice to have:Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3What we offer you:140-170k per yearUnlimited PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!


        Show more

        


        Show less",Entry level
UCLA Health,Junior Software Engineer -  Jonsson Cancer Center,"DescriptionThe newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Center is seeking a programmer/analyst with research and development experience. This position will be working with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer. Our team is passionate about applying their knowledge of software-development and design to improve scientific research. We develop scalable and distributed software solutions that maximize utilization of both local high-performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. This requires a strong background in computer science, providing a platform for technical leadership, but linked to strong personal communication and leadership skills, to help ensure insights are broadly adopted. The successful candidate will be helping us perform research that will transform the lives of cancer patients. Your responsibilities will be to use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be rewsponsible for one major and several minor projects at any point in time. We are in a rapid growth-phase, and the successful candidate will be involved in hiring of new team members. Beyond your strong inter-personal skills and computer science background, you will have experience with either systems software, databases or algorithms, linked to implementation skills at least one of C++, R, Perl or Python. You will be comfortable in UNIX/Linux environments and using continuous integration and CASE tools. Salary Range: $5525-$10925 Monthly
      

        Show more

        


        Show less",Entry level
,,,
,,,
,,,
,,,
,,,
Oshi Health,Data Engineer,"Do you love to work with data, finding ways to make it reportable, and building models that will add clinical and commercial value for the future?Do you want to bring your skills and experience to a growth stage engineering team, and help set us up for smart expansion?Are you excited by the prospect of having a high-visibility high-impact role in a fast-moving startup?Are you passionate about healthcare, and looking to create a revolutionary new approach to digestive healthcare with a radically better patient experience?If so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company.Oshi Health is revolutionizing GI care with a virtual clinic that provides easy, convenient access to a multidisciplinary care team including a GI Physician, Registered Dietician, Mental Health Professional, and Health Coach that takes a whole-person approach to diagnosing, managing and treating digestive health conditions. Our care is built on the latest evidence-based protocols and is delivered virtually through an app, secure messaging and telehealth visits with the care team. NOTE: Oshi is a fully remote company, with team members all over the US.What You’ll Do:This role will be a perfect fit if you enjoy learning the entire stack and taking on interdisciplinary challenges. A primary focus will be building out a data engineering program, including ETL, data governance, sanitization, and data operations. This part of your responsibilities will be a balance of executing data operations, and building automation to make those operations scalable.You will also have the opportunity to join engineers on the frontend end (React Native and React.js), backend (Node.js Lambdas) and Salesforce to help build the Oshi platform. Experience with any of these technologies is a plus, but more important is an enthusiasm to adapt and learn the ones that are new to you.What you’ll do: build the Oshi data programImplement and maintain data pipelines using Stitch, Databricks, and other scripting as needed to feed a PostgreSQL schema supporting Tableau reportingSupport users of Tableau by updating data sources or modifying inbound inputs as needed deliver critical BI reportsOwn the Oshi data model, ensuring that new features built and new technologies adopted serve the needs of the clinical, commercial, product, and engineering teamsManage ETL of client eligibility files and other data, to make them available for Oshi use in a secure and timely manner. Wherever possible, replace bespoke processes with automationOnce you have a understanding of Oshi’s requirements, design and implement a data strategy, (with your recommendation of approach and products,) to meet the needs of Oshi’s analytics, commercial, and clinical business linesYour work will also include:AWS maintenance and administration Writing technical documentation to outline designs for forthcoming features, outlining the implementation across all technology layersMeeting with colleagues in Strategy, Product, and Clinical to support their needs from the Engineering group.Production support responsibilities (shared with the entire engineering team) responding to alerts in Datadog, reviewing and troubleshooting issuesOur tech stack:Mobile Platforms Supported: iOS & AndroidCross-Platform Mobile Language: React NativeOther Languages: React-js, HTML, CSS, Java (Salesforce Apex), Node.js (Lambda)Systems: Salesforce, AWS Amplify / Cognito / LambdaYour Profile:A minimum of 3+ years of professional experienceBachelor's Degree or equivalent experienceGood interpersonal and relationship skills that include a positive attitudeSelf-starter who can find a way forward even when the path is unclear.Team player AND a leader simultaneously.What You’ll Bring to the Team:Passionate about creating value that changes people's livesMake low-level decisions quickly while being patient and methodical with high-level onesAre curious and passionate about digging into new technologies with a knack for picking them up quicklyAdept at prioritizing value and shipping complex products while coordinating across multiple teamsLove working with a diverse set of engineers, product managers, designers, and business partnersStrive to excel, innovate and take pride in your workWork well with other leadersAre a positive culture driverExcited about working in a fast-paced, startup cultureExperience in a regulated industry (healthcare, finance, etc.) a plusand perks:We’re revolutionizing GI care — and our employees are driving the change. We’re a hard-working and fun-loving team, committed to always learning and improving, and dedicated to doing the right thing for our members. To achieve our mission, we invest in our people:We make healthcare more equitable and accessible:Mission-driven organization focused on innovative digestive careThrive on diversity with monthly DEIB discussions, activities, and moreVirtual-first culture: Work from home anywhere in the USLive our core values: Own the outcome, Do the right thing, Be direct and open, Learn and improve, Team, Thrive on diversity We take care of our people:Competitive compensation and meaningful equityEmployer-sponsored medical, dental and vision plans Access to a “Life Concierge” through Overalls, because we know life happensTailored professional development opportunities to learn and growWe rest, recharge and re-energize:Unlimited paid time off — take what you need, when you need it13 paid company holidays to power downTeam events, such as virtual cooking classes, games, and moreRecognition of professional and personal accomplishmentsOshi Health’s Core Values:Go For ItDo the Right ThingBe Direct & OpenLearn & ImproveTEAM - Together Everyone Achieves MoreOshi Health is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Powered by JazzHRFRVWJuzRKn
      

        Show more

        


        Show less",Mid-Senior level
B Capital ,Software/Data Engineer (Entry-level),"Position SummaryThe position will focus on helping build and maintain backend integrations of our various systems (e.g., Financial System, HR System, CRM System), including Data Warehouse archiving and synchronizing. The position will also work with our Business Intelligence team to develop views/queries/tables to power Tableau Dashboards used throughout various venture capital investment lifecycle stages. Secondarily, the position will also maintain custom internal applications utilizing data from our Data Warehouse and various systems.Basic Job ResponsibilitiesImplementing processes that are fault-tolerant, resilient, and efficient in Databricks and AWS LambdaWriting clean code that is maintainable and easy to understandParticipate in code reviewsMaintain custom application plugins in WordPressUpdate our project management system, JIRA, with notes, questions, and feedback on current and future tasksBasic Job RequirementsRecent Graduate with a degree or related studies in computer science, math, or other technical field (e.g., applied mathematics, statistics, physics or engineering)Prior internship experience in similar field will be highly preferred Experience with programming languages such as Python or PHPExperience with SQL databases such as MySQL or PostgreSQLExperience with a source control system, such as GitOptional experience with Databricks, WordPress, JIRA, Cloud Computing, such as AWS Lambda/S3Ability to working in a variety of code bases and production environments.Good problem-solving skills and the ability to think of solutions.Good verbal/writing skills for explaining your work to others.Passion to grow in the role to take on AI/ML projectsCuriosity for technology and a desire to learn new skills, frameworks, and programming languages.Interest in learning financial terminology and concepts used in the Venture Capital MarketWe offer hybrid work, where most of the time will be done remote, although you are required to go into the office once a week or soThis role can be based in Los Angeles, New York and San FranciscoAt B Capital, the health and safety of our people is our number one priority. As a condition of employment all new hires are required to be fully vaccinated against Covid-19 and must show proof of such vaccination. About b CapitalB Capital is a multi-stage global investment firm that partners with extraordinary entrepreneurs to shape the future through technology. With $6.3 billion in assets under management across multiple funds, the firm focuses on seed to late-stage venture growth investments, primarily in the enterprise, financial technology and healthcare sectors. Founded in 2015, B Capital leverages an integrated team across eight locations in the US and Asia, as well as a strategic partnership with BCG, to provide the value-added support entrepreneurs need to scale fast and efficiently, expand into new markets and build exceptional companies. For more information, click here.b Capital Group Core ValuesB Honest & Trustworthy - Our people and our culture are the heart of our business. We are self-aware, supportive, and trust ourselves and each other. We speak the truth with positive intent. We hold ourselves accountable, are intellectually open, and are constantly learning and growing.B Open & Inclusive - Our diverse composition gives us broad and varied perspectives that drive better investments. We find ways to better ourselves and our communities, increasing transparency, fairness, and respect in every interaction. We thrive on the unique qualities of our people, and how together, these qualities make us special.B Collaborative - We believe in we vs I, and operate as one global team. We know that no one person has all the answers, and that we are better together. Our successes and failures are equally shared.B Bold - We take risks and understand that at times we may fail. We learn from our failures; we don’t repeat them and are constantly striving to be better.B Humble - We are humble and believe in winning together with gratitude, knowing that every finish line is the beginning of a new race. We are low ego, and lift each other up.B Persistent - When we get knocked down, we rise back up. We persevere, with the enduring perspective that only grit can help us overcome. We know our individual and collective goals and won’t stop short of achieving them.B Evolving - We innovate and advocate with boundless curiosity and creativity. We always have a startup mentality.Salary Range for NY & CA Candidates only. The actual salary will commensurate according to relevant experience. Salary expected range: $80,000 - $110,000
      

        Show more

        


        Show less",Entry level
